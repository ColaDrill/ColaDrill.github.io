---
layout:     post
title:      注意力与增强RNN
subtitle:   Attention and Augmented Recurrent Neural Networks
date:       2018-10-28
author:     Jiayue Cai
header-img: img/post-bg-debug.png
catalog: true
tags:
    - Machine Learning
    - Deep Learning
---


>Last updated on 2018-10-28... 

循环神经网络是深度学习的主要内容之一，可以处理文本、音频和视频等序列数据；可用于将序列分解为高级理解、注释序列，甚至从头开始生成新序列。

> 原英文链接：https://distill.pub/2016/augmented-rnns/

### RNN与LSTM

1.`RNN`网络示意图：
![1](https://upload-images.jianshu.io/upload_images/13187322-ced56c25d725a6f9.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/589/format/webp)

放大一点：
![2](https://upload-images.jianshu.io/upload_images/13187322-eacf9a87111333ff.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/720/format/webp)

2.传统的RNN设计与较长的序列相悖，于是就出现了 [`LSTM` “长短期记忆”网络](http://colah.github.io/posts/2015-08-Understanding-LSTMs/)
![3](https://upload-images.jianshu.io/upload_images/13187322-facc0a6e60c42276.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/722/format/webp)
![4](https://upload-images.jianshu.io/upload_images/13187322-24929eede23f9d59.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/545/format/webp)

3.后来衍生出四个大方向：

![5](https://upload-images.jianshu.io/upload_images/13187322-6e0a9881448994da.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/827/format/webp)

### [Neural Turing Machines](https://distill.pub/2016/augmented-rnns/#neural-turing-machines)

[Neural Turing Machines 神经图灵机](https://distill.pub/2016/augmented-rnns/#neural-turing-machines)将RNN与外部存储器组合在一起：
![6](https://upload-images.jianshu.io/upload_images/13187322-fc2bfd8fb8860403.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/758/format/webp)

但read和write如何运作？挑战在于我们希望使它们具有差异性。特别是，我们希望使它们在我们读取或写入的位置方面具有可区分性，以便我们可以了解读写的位置。这很棘手，因为内存地址似乎基本上是离散的。

`NTM`为此提供了一个非常聪明的解决方案：每一步，它们都可以在不同的范围内进行读写。

1.举个例子，让我们专注于阅读。RNN不是指定单个位置，而是输出“注意力分布”，描述我们如何分散我们关心不同记忆位置的值。因此，读取操作的结果是加权和。
![7](https://upload-images.jianshu.io/upload_images/13187322-fda7df71619f79d7.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/706/format/webp)

2.同样，我们一次到不同程度地写到处。同样，注意力分布描述了我们在每个位置写了多少。
我们通过使存储器中的位置的新值成为旧存储器内容和写入值的凸起组合来实现这一点，其中两者之间的位置由注意力量决定。
![8](https://upload-images.jianshu.io/upload_images/13187322-d1d65dee16aa7c76.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/712/format/webp)

3.但NTM如何决定内存中的哪些位置集中注意力呢？它们实际上使用两种不同方法的组合：基于内容的注意力和基于位置的注意力。
基于内容的关注允许NTM搜索他们的内存并专注于与他们正在寻找的内容匹配的位置，而基于位置的注意力允许内存中的相对移动，使NTM循环。
![9](https://upload-images.jianshu.io/upload_images/13187322-7ed572222c4d6f96.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/907/format/webp)

这种读写功能允许NTM执行许多以前超出神经网络的简单算法。例如，它们可以学习在内存中存储一​​个长序列，然后循环遍历它，重复重复它。
当它们这样做时，我们可以看到它们读写的地方，以便更好地了解它们正在做的事情：
![10](https://upload-images.jianshu.io/upload_images/13187322-603c539b35b326e9.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/323/format/webp)

它们还可以学习模仿查找表（lookup table），甚至学习排序数字！另一方面，它们仍然不能做很多基本的事情，比如加或加数。

自最初的NTM论文以来，已经有许多令人兴奋的论文探讨类似的方向:
- [`Neural GPU 神经GPU`](https://arxiv.org/pdf/1511.08228.pdf)克服了NTM无法增加和增加数字的能力。
- [`RL NTM`](https://arxiv.org/pdf/1505.00521.pdf) Zaremba和Sutskever使用强化学习训练NTM，而不是原始使用的可微分读/写。
- [`Neural Random Access Machines 神经随机访问机`](https://arxiv.org/pdf/1511.06392.pdf)基于指针工作。
- 一些论文探讨了可区分的数据结构，如`堆栈和队列`[1](http://papers.nips.cc/paper/5648-learning-to-transduce-with-unbounded-memory.pdf)[2](http://papers.nips.cc/paper/5857-inferring-algorithmic-patterns-with-stack-augmented-recurrent-nets.pdf)。`内存网络`[1](https://arxiv.org/pdf/1410.3916.pdf)[2](https://arxiv.org/pdf/1506.07285.pdf)是另一种攻击类似问题的方法。

在某种客观意义上，这些模型可以执行的许多任务，例如学习如何添加数字。神经网络还能够做很多其他的事情，但是像神经图灵机这样的模型似乎已经对它的能力产生了极大的限制。

这些模型有许多开源实现。NTM的开源实现包括[Taehoon Kim（TensorFlow）](https://github.com/carpedm20/NTM-tensorflow)，[Shawn Tan（Theano）](https://github.com/shawntan/neural-turing-machines)，[Fumin（Go）](https://github.com/fumin/ntm)，[Kai Sheng Tai（Torch）](https://github.com/kaishengtai/torch-ntm)和[Snip（Lasagne）](https://github.com/snipsco/ntm-lasagne)。
Neural GPU的代码是开源的，并放在[TensorFlow模型库](https://github.com/tensorflow/models/tree/master/neural_gpu)中。
Memory Networks的开源实现包括[Facebook（Torch / Matlab）](https://github.com/facebook/MemNN)，[YerevaNN（Theano）](https://github.com/YerevaNN/Dynamic-memory-networks-in-Theano)和[Taehoon Kim（TensorFlow）](https://github.com/carpedm20/MemN2N-tensorflow)。

### Attentional Interfaces

未完待续...














