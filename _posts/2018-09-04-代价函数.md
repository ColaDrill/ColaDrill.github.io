---
layout:     post
title:      代价函数
subtitle:   Cost Function
date:       2018-09-04
author:     Jiayue Cai
header-img: img/post-bg-debug.png
catalog: true
tags:
    - Machine Learning
---


> Last updated on 2019-4-3... 

Cost Function和Loss Function的区别：
- Cost Function：指基于参数w和b，在所有训练样本上的总成本
- Loss Function：指单个训练样本的损失函数

> [各种概率分布](http://blog.lisp4fun.com/2017/11/11/pdf)

![](/img/post/20180904/0.png)

### 均方误差 MSE

> 假设是高斯分布，又名正态分布

均方误差的含义是求一个batch中n个样本的n个输出与期望输出的差的平方的平均值。

回归问题中常用的损失函数式均方误差(MSE,mean squared error)，定义如下：

![](/img/post/20180904/1.png)

### 交叉熵 Cross entropy

> 假设是伯努利分布，又名0-1分布。

> [《深入理解交叉熵 / 对数损失》](https://zhuanlan.zhihu.com/p/52100927)。

分类问题中，预测结果是（或可以转化成）输入样本属于n个不同分类的对应概率。
比如对于一个4分类问题，期望输出应该为 g0=[0,1,0,0]，实际输出为 g1=[0.2,0.4,0.4,0]。
计算g1与g0之间的差异所使用的方法，就是损失函数，分类问题中常用损失函数是交叉熵。

**对数损失：**
![](/img/post/20180904/5.png)
其中：
![](/img/post/20180904/6.png)

将sigmoid代入对数损失，于是得到了交叉熵。（[>>推导过程](https://blog.csdn.net/google19890102/article/details/79496256)）

** 交叉熵 **描述的是两个概率分布之间的距离，距离越小表示这两个概率越相近，越大表示两个概率差异越大。对于两个概率分布 p 和 q ，使用 q 来表示 p 的交叉熵为：
![](/img/post/20180904/2.png)

上式表示的物理意义是使用概率分布 q 来表示概率分布 p 的困难程度，q 是预测值，p 是期望值。

> 由公式可以看出来，p 与 q 之间的交叉熵 和 q 与 p 之间的交叉熵不是等价的。

> 神经网络的输出，也就是前向传播的输出可以通过Softmax变成概率分布，之后就可以使用交叉熵函数计算损失了。

#### 逻辑回归中的交叉熵

- MSE的假设是高斯分布，交叉熵的假设是伯努利分布，而逻辑回归采用的就是伯努利分布
- MSE会导致代价函数J(θ)非凸，这会存在很多局部最优解，而我们更想要代价函数是凸函数
- MSE相对于交叉熵而言会加重梯度弥散（MSE的梯度是交叉熵梯度的1/4）

> 以上解释的参考链接：[Poll的笔记](http://www.cnblogs.com/maybe2030/p/9163479.html)

单样本交叉熵：
![](/img/post/20180904/4.png)

多样本交叉熵：
![](/img/post/20180904/3.png)

> y代表期望输出（实际值），y带上标 代表模型实际输出（预测值）

> [LR中从预测函数到目标函数再到梯度下降的推导](https://blog.csdn.net/ZesenChen/article/details/79589990)

### 其他常见损失函数

#### 0-1损失函数

> **感知机模型采用**

![](/img/post/20180904/7.png)

当预测错误时，损失函数值为1，预测正确时，损失函数值为0。

该损失函数不考虑预测值和真实值的误差程度，也就是只要预测错误，预测错误差一点和差很多是一样的。

但是由于相等这个条件太过严格，因此我们可以放宽条件，即满足差绝对值小于T时认为相等：
![](/img/post/20180904/8.png)

#### 绝对值损失函数(absolute，MAE)

![](/img/post/20180904/9.png)
MAE相对MSE来说，差距不会被平方放大。

#### 对数损失函数(logarithmic)

![](/img/post/20180904/10.png)

该损失函数用到了极大似然估计的思想。

P(Y|X)通俗的解释：在当前模型的基础上，对于样本X，其预测值为Y，也就是预测正确的概率。

由于概率之间的同时满足需要使用乘法，为了将其转化为加法，我们将其取对数。

最后由于是损失函数，所以预测正确的概率越高，其损失值应该是越小，因此再加个负号取个反。

#### 指数损失函数(Exponential)

> **AdaBoost模型采用**

![](/img/post/20180904/11.png)

#### Hinge损失函数

> **SVM模型采用**

Hinge loss用于最大间隔（maximum-margin）分类，其中最有代表性的就是支持向量机SVM。

标准形式：
![](/img/post/20180904/12.png)
其中，t为目标值（-1或+1），y是分类器输出的预测值，并不直接是类标签。
其含义为，当t和y的符号相同时（表示y预测正确）并且|y|≥1时，hinge loss为0；当t和y的符号相反时，hinge loss随着y的增大线性增大。

与上面统一的形式：
![](/img/post/20180904/13.png)

### 总结

![](/img/post/20180904/14.png)

> 更多损失函数:[link1](https://segmentfault.com/a/1190000015320388)、[link2](https://redstonewill.com/1584/)











