---
layout:     post
title:      代价函数
subtitle:   Cost Function
date:       2018-09-04
author:     Jiayue Cai
header-img: img/post-bg-debug.png
catalog: true
tags:
    - Machine Learning
---


> Last updated on 2018-9-6... 

Cost Function和Loss Function的区别：
- Cost Function：指基于参数w和b，在所有训练样本上的总成本
- Loss Function：指单个训练样本的损失函数

### 均方误差 MSE

> 假设是高斯分布，又名正态分布

均方误差的含义是求一个batch中n个样本的n个输出与期望输出的差的平方的平均值。

回归问题中常用的损失函数式均方误差(MSE,mean squared error)，定义如下：

![](/img/post/20180904/1.png)

### 交叉熵 Cross entropy

> 假设是伯努利分布，又名0-1分布。

分类问题中，预测结果是（或可以转化成）输入样本属于n个不同分类的对应概率。比如对于一个4分类问题，期望输出应该为 g0=[0,1,0,0]，实际输出为 g1=[0.2,0.4,0.4,0]。

计算g1与g0之间的差异所使用的方法，就是损失函数，分类问题中常用损失函数是交叉熵。

交叉熵（cross entropy）描述的是两个概率分布之间的距离，距离越小表示这两个概率越相近，越大表示两个概率差异越大。对于两个概率分布 p 和 q ，使用 q 来表示 p 的交叉熵为：
![](/img/post/20180904/2.png)

上式表示的物理意义是使用概率分布 q 来表示概率分布 p 的困难程度，q 是预测值，p 是期望值。

> 由公式可以看出来，p 与 q 之间的交叉熵 和 q 与 p 之间的交叉熵不是等价的。

> 神经网络的输出，也就是前向传播的输出可以通过Softmax变成概率分布，之后就可以使用交叉熵函数计算损失了。

#### 逻辑回归中的交叉熵

- MSE的假设是高斯分布，交叉熵的假设是伯努利分布，而逻辑回归采用的就是伯努利分布
- MSE会导致代价函数J(θ)非凸，这会存在很多局部最优解，而我们更想要代价函数是凸函数
- MSE相对于交叉熵而言会加重梯度弥散（MSE的梯度是交叉熵梯度的1/4）

> 以上解释的参考链接：[Poll的笔记](http://www.cnblogs.com/maybe2030/p/9163479.html)

多样本交叉熵：
![](/img/post/20180904/3.png)

单样本交叉熵：
![](/img/post/20180904/4.png)

> y代表期望输出（实际值），y带上标 代表模型实际输出（预测值）

> [LR中从预测函数到目标函数再到梯度下降的推导](https://blog.csdn.net/ZesenChen/article/details/79589990)







