---
layout:     post
title:      深度强化学习综述
subtitle:   Summary of Deep Reinforcement Learning
date:       2018-10-21
author:     Jiayue Cai
header-img: img/post-bg-black.jpg
catalog: true
tags:
    - Deep Learning
    - Reinforcement Learning
---


>>Last updated on 2018-10-23... 

由于第一次接触强化学习，难免看到一些专有名词会犯难，所以本次先选择了来自苏州大学的一篇[《深度强化学习综述》](http://cjc.ict.ac.cn/online/cre/lq-2017119103322.pdf)，看一看国人对专有名词的翻译以及对DRL的理解。

### 引言

`DL` 的基本思想是通过多层的网络结构和非线性变换，组合低层特征，形成抽象的、易于区分的高层表示，以发现数据的分布式特征表示。因此 DL 方法`侧重于对事物的感知和表达`。

`RL` 的基本思想是通过最大化智能体（agent）从环境中获得的累计奖赏值，以学习到完成目标的最优策略。因此 RL 方法更加`侧重于学习解决问题的策略`。

随着人类社会的飞速发展，在越来越多复杂的现实场景任务中，需要利用 DL 来自动学习大规模输入数据的抽象表征，并以此表征为依据进行自我激励的 RL，优化解决问题的策略。

其**学习过程**可以描述为：

1. 在每个时刻 agent与环境交互得到一个高维度的观察，并利用 DL 方法来感知观察，以得到抽象、具体的状态特征表示；

2. 基于预期回报来评价各动作的价值函数，并通过某种策略将当前状态映射为相应的动作。

3. 环境对此动作做出反应，并得到下一个观察。通过不断循环以上过程，最终可以得到实现目标的最优策略。

**DRL原理框架**如图 1 所示:
![1](https://upload-images.jianshu.io/upload_images/13187322-429d2fc8e1a3531b.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/240/format/webp)

目前 DRL 技术在`游戏`、`机器人控制`、`参数优化`、`机器视觉`等领域中得到了广泛的应用，并被认为是迈向通用人工智能（ArtificialGeneral Intelligence，AGI）的重要途径。

本文对 DRL的研究历程和发展现状进行了详细的阐述。整体架构如图 2 所示:
![2](https://upload-images.jianshu.io/upload_images/13187322-d28b8e7d32f0fdd8.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/554/format/webp)

### 基于值函数的DRL

#### [深度Q网络（DQN）](https://www.cs.toronto.edu/~vmnih/docs/dqn.pdf)

Deep Q-Network模型用于处理基于视觉感知的控制任务，是 DRL 领域的开创性工作。
![3](https://upload-images.jianshu.io/upload_images/13187322-4cb8fb8c712d1a23.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/382/format/webp)
![4](https://upload-images.jianshu.io/upload_images/13187322-ab5eae017a28a074.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/505/format/webp)

- 在训练过程中使用`经验回放机制`（experience replay），在线处理得到的转移样本。
- 除了使用深度卷积网络近似表示当前的值函数之外，还单独使用了另一个网络来产生`目标 Q 值`。当前值网络的参数 θ 是实时更新的，每经过 N 轮迭代，将当前值网络的参数复制给目标值网络。通过最小化当前 Q 值和目标 Q 值之间的均方误差来更新网络参数。引入目标值网络后，在一段时间内目标 Q 值是保持不变的，一定程度上降低了当前 Q 值和目标 Q 值之间的相关性，提升了算法的稳定性。
- 将奖赏值和误差项缩小到有限的区间内，保证了 Q 值和梯度值都处于合理的范围内，提高了算法的稳定性。

#### [深度双Q网络（DDQN）](http://www0.cs.ucl.ac.uk/staff/d.silver/web/Applications_files/doubledqn.pdf)

Deep Double Q-Network的作者发现并证明了传统的DQN普遍会过高估计Action的Q值，而且估计误差会随Action的个数增加而增加。如果高估不是均匀的，则会导致某个次优的Action高估的Q值超过了最优Action的Q值，永远无法找到最优的策略。
![5](https://upload-images.jianshu.io/upload_images/13187322-7c89f9e914975ac8.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/450/format/webp)

- 在双Q学习中有两套不同的参数： θ 和 θ-。其中 θ 用来选择对应最大 Q 值的动作，θ- 用来评估最优动作的 Q 值。两套参数将动作选择和策略评估分离开，降低了过高估计 Q 值的风险。
- DDQN使用当前值网络的参数来选择最优动作，使用目标值网络的参数θ- 来评估该最优动作。
- DDQN 在其他方面都与 DQN 保持一致。实验表明，DDQN 能够估计出更加准确的Q 值。

#### [基于优势学习的深度Q网络](https://arxiv.org/abs/1512.04860)

降低 Q 值的评估误差可以提升性能。Bellemare 等人在贝尔曼方程中定义新的操作符，来增大最优动作值和次优动作值之间的差异，以缓和每次都选取下一状态中最大 Q 值对应动作所带来的评估误差。
![6](https://upload-images.jianshu.io/upload_images/13187322-814f775ce2f653ec.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/500/format/webp)

- 根据`优势学习`（Advantage Learning，AL）定义两种新的操作符，用`AL和PAL误差项`来替代贝尔曼方程中的误差项，可以有效地增加最优和次优动作对应值函数之间的差异，从而获得更加精确的 Q值。
- 优势学习相当于一个回归任务，每一步目标是最小化与Rt的距离，整体来看就是之前策略累计奖励的动态平均。

#### [基于优先级采样的深度Q网络](https://arxiv.org/pdf/1511.05952.pdf)

在每个时刻，经验回放机制从样本池中等概率地抽取小批量的样本用于训练.然而`等概率采样并不能区分不同样本的重要性`。

- 该抽样方法将每个样本的时间差分（Temporal Difference, TD）误差项作为评价优先级的标准。其绝对值越大，对应样本被采样的概率越高。
- 在抽样过程中该方法使用随机比例化（stochastic prioritization）和重要性采样权重（importance-sampling weights）两种技巧。
	- 随机比例化操作不仅能充分利用较大 TD 误差项对应的样本，而且保证了抽取样本的多样性。 
	- 重要性采样权重的使用放缓了参数更新的速度，保证了学习的稳定性。
- 实验表明，基于该抽样方式的深度双Q 网络可以提升训练速度，并在很多 Atari 2600 游戏中获得了更高的分数。

> `^以上是基于训练算法的改进`

#### [基于竞争架构的DQN](https://arxiv.org/pdf/1511.06581.pdf)

DQN 将 CNN 提取的抽象特征经过全连接层后，直接在输出层输出对应动作的 Q 值，而引入竞争网络结构的模型则将 CNN 提取的抽象特征分流到两个支路中，其中一路代表状态值函数，另一路代表依赖状态的动作优势函数（advantage function）。即`用竞争网络替代了之前的全连接层`。
![7](https://upload-images.jianshu.io/upload_images/13187322-820a26f30ca575da.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/402/format/webp)

- 通过该种竞争网络结构，agent 可以在策略评估过程中更快地识别出正确的行为。
- 实际操作中，一般要将动作优势流设置为单独动作优势函数值减去某状态下所有动作优势函数的平均值。该技巧不仅可以保证该状态下各动作的优势函数相对排序不变，而且可以缩小 Q 值的范围，去除多余的自由度。

#### 深度循环Q网络

在传统的 RL 方法中，状态信息的部分可观察性一直是个亟待解决的难题。
DQN 通过堆叠离当前时刻最近的 4 幅历史图像组成输入状态，有效缓解了状态信息的部分可观察问题，却增加了网络的计算和存储负担。
Hausknecht 等人利用循环神经网络结构来记忆时间轴上连续的历史状态信息，提出了`DRQN 模型`。
![8](https://upload-images.jianshu.io/upload_images/13187322-a59d9081cb6afac3.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/425/format/webp)

- DRQN 将 DQN 中第 1 个`全连接层`的部件`替换成`了 256 个长短期记忆单元（Long Short-Term Memory，`LSTM`）。此时模型的输入仅为当前时刻的一幅图像，减少了深度网络感知图像特征所耗费的计算资源
- 此时模型的输入仅为当前时刻的一幅图像，减少了深度网络感知图像特征所耗费的计算资源.

> `^以上是基于网络结构的改进`

### 基于策略梯度的DRL

### 基于搜索与监督的DRL








