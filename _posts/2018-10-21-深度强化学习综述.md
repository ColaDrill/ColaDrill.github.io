---
layout:     post
title:      深度强化学习综述
subtitle:   Summary of Deep Reinforcement Learning
date:       2018-10-21
author:     Jiayue Cai
header-img: img/post-bg-black.jpg
catalog: true
tags:
    - Deep Learning
    - Reinforcement Learning
---


>>Last updated on 2018-10-21... 

由于第一次接触强化学习，难免看到一些专有名词会犯难，所以本次先选择了来自苏州大学的一篇[《深度强化学习综述》](http://cjc.ict.ac.cn/online/cre/lq-2017119103322.pdf)，看一看国人对专有名词的翻译以及对DRL的理解。

### 引言

`DL` 的基本思想是通过多层的网络结构和非线性变换，组合低层特征，形成抽象的、易于区分的高层表示，以发现数据的分布式特征表示。因此 DL 方法`侧重于对事物的感知和表达`。

`RL` 的基本思想是通过最大化智能体（agent）从环境中获得的累计奖赏值，以学习到完成目标的最优策略。因此 RL 方法更加`侧重于学习解决问题的策略`。

随着人类社会的飞速发展，在越来越多复杂的现实场景任务中，需要利用 DL 来自动学习大规模输入数据的抽象表征，并以此表征为依据进行自我激励的 RL，优化解决问题的策略。

其**学习过程**可以描述为：

1. 在每个时刻 agent与环境交互得到一个高维度的观察，并利用 DL 方法来感知观察，以得到抽象、具体的状态特征表示；

2. 基于预期回报来评价各动作的价值函数，并通过某种策略将当前状态映射为相应的动作。

3. 环境对此动作做出反应，并得到下一个观察。通过不断循环以上过程，最终可以得到实现目标的最优策略。

**DRL原理框架**如图 1 所示:
![1](https://upload-images.jianshu.io/upload_images/13187322-429d2fc8e1a3531b.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/240/format/webp)

目前 DRL 技术在`游戏`、`机器人控制`、`参数优化`、`机器视觉`等领域中得到了广泛的应用，并被认为是迈向通用人工智能（ArtificialGeneral Intelligence，AGI）的重要途径。

本文对 DRL的研究历程和发展现状进行了详细的阐述。整体架构如图 2 所示:
![2](https://upload-images.jianshu.io/upload_images/13187322-d28b8e7d32f0fdd8.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/554/format/webp)

### 基于值函数的深度强化学习

#### 深度Q网络（DQN）

Deep Q-Network模型用于处理基于视觉感知的控制任务，是 DRL 领域的开创性工作。
![3](https://upload-images.jianshu.io/upload_images/13187322-ab5eae017a28a074.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/505/format/webp)

- 在训练过程中使用`经验回放机制`（experience replay），在线处理得到的转移样本。
- 除了使用深度卷积网络近似表示当前的值函数之外，还单独使用了另一个网络来产生`目标 Q 值`。当前值网络的参数 θ 是实时更新的，每经过 N 轮迭代，将当前值网络的参数复制给目标值网络。通过最小化当前 Q 值和目标 Q 值之间的均方误差来更新网络参数。引入目标值网络后，在一段时间内目标 Q 值是保持不变的，一定程度上降低了当前 Q 值和目标 Q 值之间的相关性，提升了算法的稳定性。
- 将奖赏值和误差项缩小到有限的区间内，保证了 Q 值和梯度值都处于合理的范围内，提高了算法的稳定性。

#### [深度双Q网络（DDQN）](http://www0.cs.ucl.ac.uk/staff/d.silver/web/Applications_files/doubledqn.pdf)

Deep Double Q-Network的作者发现并证明了传统的DQN普遍会过高估计Action的Q值，而且估计误差会随Action的个数增加而增加。如果高估不是均匀的，则会导致某个次优的Action高估的Q值超过了最优Action的Q值，永远无法找到最优的策略。
![4](https://upload-images.jianshu.io/upload_images/13187322-7c89f9e914975ac8.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/450/format/webp)

- 在双Q学习中有两套不同的参数： θ 和 θ-。其中 θ 用来选择对应最大 Q 值的动作，θ- 用来评估最优动作的 Q 值。两套参数将动作选择和策略评估分离开，降低了过高估计 Q 值的风险。
- DDQN使用当前值网络的参数来选择最优动作，使用目标值网络的参数θ- 来评估该最优动作。
- DDQN 在其他方面都与 DQN 保持一致。实验表明，DDQN 能够估计出更加准确的Q 值。





















