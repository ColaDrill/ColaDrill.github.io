---
layout:     post
title:      深度强化学习综述
subtitle:   Summary of Deep Reinforcement Learning
date:       2018-10-21
author:     Jiayue Cai
header-img: img/post-bg-black.jpg
catalog: true
tags:
    - Deep Learning
    - Reinforcement Learning
---


>>Last updated on 2018-10-23... 

由于第一次接触强化学习，难免看到一些专有名词会犯难，所以本次先选择了来自苏州大学的一篇[《深度强化学习综述》](http://cjc.ict.ac.cn/online/cre/lq-2017119103322.pdf)，看一看国人对专有名词的翻译以及对DRL的理解。

**下面点击相应的标题即可阅读相应的完整论文**

### 引言

`DL` 的基本思想是通过多层的网络结构和非线性变换，组合低层特征，形成抽象的、易于区分的高层表示，以发现数据的分布式特征表示。因此 DL 方法`侧重于对事物的感知和表达`。

`RL` 的基本思想是通过最大化智能体（agent）从环境中获得的累计奖赏值，以学习到完成目标的最优策略。因此 RL 方法更加`侧重于学习解决问题的策略`。

随着人类社会的飞速发展，在越来越多复杂的现实场景任务中，需要利用 DL 来自动学习大规模输入数据的抽象表征，并以此表征为依据进行自我激励的 RL，优化解决问题的策略。

其**学习过程**可以描述为：

1. 在每个时刻 agent与环境交互得到一个高维度的观察，并利用 DL 方法来感知观察，以得到抽象、具体的状态特征表示；

2. 基于预期回报来评价各动作的价值函数，并通过某种策略将当前状态映射为相应的动作。

3. 环境对此动作做出反应，并得到下一个观察。通过不断循环以上过程，最终可以得到实现目标的最优策略。

**DRL原理框架**如图 1 所示:
![1](https://upload-images.jianshu.io/upload_images/13187322-429d2fc8e1a3531b.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/240/format/webp)

目前 DRL 技术在`游戏`、`机器人控制`、`参数优化`、`机器视觉`等领域中得到了广泛的应用，并被认为是迈向通用人工智能（ArtificialGeneral Intelligence，AGI）的重要途径。

本文对 DRL的研究历程和发展现状进行了详细的阐述。整体架构如图 2 所示:
![2](https://upload-images.jianshu.io/upload_images/13187322-d28b8e7d32f0fdd8.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/554/format/webp)

### 基于值函数的DRL

#### [深度Q网络（DQN）](https://www.cs.toronto.edu/~vmnih/docs/dqn.pdf)

Deep Q-Network模型用于处理基于视觉感知的控制任务，是 DRL 领域的开创性工作。
![3](https://upload-images.jianshu.io/upload_images/13187322-4cb8fb8c712d1a23.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/382/format/webp)
![4](https://upload-images.jianshu.io/upload_images/13187322-ab5eae017a28a074.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/505/format/webp)

- 在训练过程中使用`经验回放机制`（experience replay），在线处理得到的转移样本。
- 除了使用深度卷积网络近似表示当前的值函数之外，还单独使用了另一个网络来产生`目标 Q 值`。当前值网络的参数 θ 是实时更新的，每经过 N 轮迭代，将当前值网络的参数复制给目标值网络。通过最小化当前 Q 值和目标 Q 值之间的均方误差来更新网络参数。引入目标值网络后，在一段时间内目标 Q 值是保持不变的，一定程度上降低了当前 Q 值和目标 Q 值之间的相关性，提升了算法的稳定性。
- 将奖赏值和误差项缩小到有限的区间内，保证了 Q 值和梯度值都处于合理的范围内，提高了算法的稳定性。

#### [深度双Q网络（DDQN）](http://www0.cs.ucl.ac.uk/staff/d.silver/web/Applications_files/doubledqn.pdf)

Deep Double Q-Network的作者发现并证明了传统的DQN普遍会过高估计Action的Q值，而且估计误差会随Action的个数增加而增加。如果高估不是均匀的，则会导致某个次优的Action高估的Q值超过了最优Action的Q值，永远无法找到最优的策略。
![5](https://upload-images.jianshu.io/upload_images/13187322-7c89f9e914975ac8.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/450/format/webp)

- 在双Q学习中有两套不同的参数： θ 和 θ-。其中 θ 用来选择对应最大 Q 值的动作，θ- 用来评估最优动作的 Q 值。两套参数将动作选择和策略评估分离开，降低了过高估计 Q 值的风险。
- DDQN使用当前值网络的参数来选择最优动作，使用目标值网络的参数θ- 来评估该最优动作。
- DDQN 在其他方面都与 DQN 保持一致。实验表明，DDQN 能够估计出更加准确的Q 值。

#### [基于优势学习的深度Q网络](https://arxiv.org/abs/1512.04860)

降低 Q 值的评估误差可以提升性能。Bellemare 等人在贝尔曼方程中定义新的操作符，来增大最优动作值和次优动作值之间的差异，以缓和每次都选取下一状态中最大 Q 值对应动作所带来的评估误差。
![6](https://upload-images.jianshu.io/upload_images/13187322-814f775ce2f653ec.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/500/format/webp)

- 根据`优势学习`（Advantage Learning，AL）定义两种新的操作符，用`AL和PAL误差项`来替代贝尔曼方程中的误差项，可以有效地增加最优和次优动作对应值函数之间的差异，从而获得更加精确的 Q值。
- 优势学习相当于一个回归任务，每一步目标是最小化与Rt的距离，整体来看就是之前策略累计奖励的动态平均。

#### [基于优先级采样的深度Q网络](https://arxiv.org/pdf/1511.05952.pdf)

在每个时刻，经验回放机制从样本池中等概率地抽取小批量的样本用于训练.然而`等概率采样并不能区分不同样本的重要性`。

- 该抽样方法将每个样本的时间差分（Temporal Difference, TD）误差项作为评价优先级的标准。其绝对值越大，对应样本被采样的概率越高。
- 在抽样过程中该方法使用随机比例化（stochastic prioritization）和重要性采样权重（importance-sampling weights）两种技巧。
	- 随机比例化操作不仅能充分利用较大 TD 误差项对应的样本，而且保证了抽取样本的多样性。 
	- 重要性采样权重的使用放缓了参数更新的速度，保证了学习的稳定性。
- 实验表明，基于该抽样方式的深度双Q 网络可以提升训练速度，并在很多 Atari 2600 游戏中获得了更高的分数。

> `^以上是基于训练算法的改进`

#### [基于竞争架构的DQN](https://arxiv.org/pdf/1511.06581.pdf)

DQN 将 CNN 提取的抽象特征经过全连接层后，直接在输出层输出对应动作的 Q 值，而引入竞争网络结构的模型则将 CNN 提取的抽象特征分流到两个支路中，其中一路代表状态值函数，另一路代表依赖状态的动作优势函数（advantage function）。即`用竞争网络替代了之前的全连接层`。
![7](https://upload-images.jianshu.io/upload_images/13187322-820a26f30ca575da.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/402/format/webp)

- 通过该种竞争网络结构，agent 可以在策略评估过程中更快地识别出正确的行为。
- 实际操作中，一般要将动作优势流设置为单独动作优势函数值减去某状态下所有动作优势函数的平均值。该技巧不仅可以保证该状态下各动作的优势函数相对排序不变，而且可以缩小 Q 值的范围，去除多余的自由度。

#### [深度循环Q网络](https://arxiv.org/pdf/1507.06527.pdf)

在传统的 RL 方法中，状态信息的部分可观察性一直是个亟待解决的难题。
DQN 通过堆叠离当前时刻最近的 4 幅历史图像组成输入状态，有效缓解了状态信息的部分可观察问题，却增加了网络的计算和存储负担。
Hausknecht 等人利用循环神经网络结构来记忆时间轴上连续的历史状态信息，提出了`DRQN 模型`。
![8](https://upload-images.jianshu.io/upload_images/13187322-a59d9081cb6afac3.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/425/format/webp)

- DRQN 将 DQN 中第 1 个`全连接层`的部件`替换成`了 256 个长短期记忆单元（Long Short-Term Memory，`LSTM`）。此时模型的输入仅为当前时刻的一幅图像，减少了深度网络感知图像特征所耗费的计算资源
- 此时模型的输入仅为当前时刻的一幅图像，减少了深度网络感知图像特征所耗费的计算资源.

> `^以上是基于网络结构的改进`

### 基于策略梯度的DRL

策略梯度是一种常用的策略优化方法，它通过不断计算策略期望总奖赏关于策略参数的梯度来更新策略参数，最终收敛于最优策略。
因此在解决 DRL 问题时，可以采用参数为的深度神经网络来进行参数化表示策略，并利用策略梯度方法来优化策略。

在求解 DRL 问题时，往往第一选择是采取基于策略梯度的算法。原因是它能够直接优化策略的期望总奖赏，并以端对端的方式直接在策略空间中搜索最优策略，省去了繁琐的中间环节。
因此与 DQN 及其改进模型相比，基于策略梯度的 DRL 方法适用范围更广，策略优化的效果也更好。

假设一个完整情节的状态、动作和奖赏的轨迹为：
![9](https://upload-images.jianshu.io/upload_images/13187322-b16ebc697f70e635.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/359/format/webp)
则策略梯度表示为如下的形式：
![10](https://upload-images.jianshu.io/upload_images/13187322-94bf2e348c322513.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/231/format/webp)

#### [区域信赖的策略最优化（TRPO）](https://arxiv.org/pdf/1502.05477.pdf)

> **通过各种策略梯度方法直接优化用深度神经网络参数化表示的策略**

![11](https://upload-images.jianshu.io/upload_images/13187322-7bad7fd7744adf17.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/533/format/webp)

Trust Region Policy Optimization的核心思想是：
- `强制限制`同一批次数据上新旧两种策略预测分布的` KL 差异`，从而避免导致策略发生太大改变的参数更新步。
- 为了将应用范围扩展到大规模状态空间的 DRL 任务中，TRPO 算法使用深度神经网络来参数化策略，在只接收原始输入图像的情况下实现了端对端的控制。

#### [基于行动者评论家](https://arxiv.org/pdf/1509.02971.pdf)

TRPO这类方法在每个迭代步，都需要采样批量大小为 N 的轨迹来更新策略梯度。然而在许多复杂的现实场景中，很难在线获得大量训练数据。

Lillicrap 等人利用 DQN 扩展 Q 学习算法的思路对确定性策略梯度（Deterministic Policy Gradient，DPG）方法进行改造，提出了一种`基于 AC 框架的深度确定性策略梯度`（Deep Deterministic Policy Gradient，`DDPG`）算法。
- 该算法可用于解决连续动作空间上的 DRL 问题.DDPG分别使用参数为和的深度神经网络来表示确定性策略和值函数。
- 其中，`策略网络`用来更新策略，对应 AC 框架中的行动者；`值网络`用来逼近状态动作对的值函数，并提供梯度信息，对应 AC 框架中的评论家。

![12](https://upload-images.jianshu.io/upload_images/13187322-a9c40a20f36d6431.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/348/format/webp)

实验表明，DDPG 不仅在一系列连续动作空间的任务中表现稳定，而且求得最优解所需要的时间步也远远少于 DQN。与基于值函数的 DRL 方法相比，基于 AC 框架的深度策略梯度方法优化策略效率更高、求解速度更快。

#### [异步的优势行动者评论家算法（A3C）](https://arxiv.org/pdf/1602.01783.pdf)

经验回放机制存在两个不足之处：
- agent 与环境的每次实时交互都需要耗费很多的内存和计算力
- 经验回放机制要求 agent 采用离策略（off-policy）方法来进行学习，而离策略方法只能基于旧策略生成的数据进行更新

![13](https://upload-images.jianshu.io/upload_images/13187322-3d02f81bc9e6e1bd.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/483/format/webp)

异步的优势行动者评论家算法（Asynchronous Advantage Actor-Critic，A3C）在各类连续动作空间的控制任务上表现的最好。
- A3C 算法`利用 CPU 多线程的功能并行`、`异步地执行多个 agent`。
- 因此在任意时刻，并行的 agent 都将会经历许多不同的状态，去除了训练过程中产生的状态转移样本之间的关联性，因此这种低消耗的异步执行方式可以很好地替代经验回放机制。

### [基于搜索与监督的DRL](https://www.nature.com/articles/nature16961)

除了基于值函数的 DRL 和基于策略梯度的DRL 之外，还可以通过增加额外的人工监督来促进策略搜索的过程，即为基于搜索与监督的 DRL 的核心思想。

`蒙特卡洛树搜索`（Monte Carlo Tree Search，MCTS)作为一种经典的启发式策略搜索方法，被广泛用于`游戏博弈问题（AlphaGO）`中的行动规划。

在基于搜索与监督的 DRL 方法中，策略搜索一般是通过` MCTS `来完成的。

在 AI 领域中，由于围棋存在状态空间巨大且精确评估棋盘布局、走子困难等原因，开发出一个能够精通围棋游戏的 agent，一直被认为是最有挑战性的难题.直到 Silver 等人将 CNN 与 MCTS 相结合，提出了一种被称作为 AlphaGo 的围棋算法，在一定程度上解决了这一难题.AlphaGo 的主要思想有两点：
- 使用 MCTS 来近似估计每个状态的值函数
- 使用基于值函数的 CNN 来评估棋盘的当前布局和走子.AlphaGo 完整的学习系统主要由以下 4 个部分组成：
	- 策略网络（policy network）.又分为监督学习的策略网络和 RL 的策略网络.策略网络的作用是根据当前的局面来预测和采样下一步走棋
	- 滚轮策略（rollout policy）.目标也是预测下一步走子，但是预测的速度是策略网络的 1000倍
	- 估值网络（value network）.根据当前局面，估计双方获胜的概率
	- MCTS将策略网络、滚轮策略和估值网络融合进策略搜索的过程中，以形成一个完整的系统

### 分层DRL（HRL）

在一些复杂的 DRL 任务中，直接以最终目标为导向来优化策略，效率很低。
因此可以利用分层强化学习（Hierarchical Reinforcement Learning，HRL）将最终目标分解为多个子任务来学习层次化的策略，并通过组合多个子任务的策略形成有效的全局策略。

本章将主要介绍 3 种具有代表性的分层 DRL 算法。

#### [基于时空抽象和内在激励的HRL （h-DQN）](https://papers.nips.cc/paper/6233-hierarchical-deep-reinforcement-learning-integrating-temporal-abstraction-and-intrinsic-motivation.pdf)

在一些复杂的目标导向型任务中，`稀疏反馈的问题`一直阻碍着 agent 性能的提升。现有的各种DRL 模型（DQN、DRQN 等）在面对操作难度很大的 Montezuma's Revenge 游戏时，并不能表现出任何的智能行为。

由于在学习过程中，agent得到的反馈信号极少，导致其对某些重要状态空间的探索很不充分。若要在此类复杂的环境中进行有效的学习，agent 必须感知出`层次化时空抽象`（temporal abstraction）的知识表达，并在此基础上通过某些内在激励来促进其探索。

层次化的 DQN 算法（hierarchical Deep Q-Network，`h-DQN`）
- h-DQN是一种基于时空抽象和内在激励的分层 DRL算法，通过在不同的时空尺度上设置子目标来层次化值函数。
- 顶层的值函数用于确定 agent 的决策，以得到下一个内在激励的子目标，而底层的值函数用于确定 agent 的行动，以满足顶层的子目标。

![14](https://upload-images.jianshu.io/upload_images/13187322-9e1774751c23b069.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/233/format/webp)















