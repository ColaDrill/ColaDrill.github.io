---
layout:     post
title:      生成对抗用户模型
subtitle:   长期兴趣、Cascading-DQN算法（GAN & RL）
date:       2019-05-21
author:     Jiayue Cai
header-img: img/post-bg-alibaba.jpg
catalog: true
tags:
    - Recommending System
---


> Last updated on 2019-6-4...

这个暑期马上要去蚂蚁金服实习了，然后近期就偶然发现了蚂蚁金服AI团队最新发表的推荐领域的论文《Generative Adversarial User Model for Reinforcement Learning Based Recommendation System》ICML 2019，有种妙不可言的感觉~

> [论文链接](http://proceedings.mlr.press/v97/chen19f/chen19f.pdf)、[主要参考链接](https://zhuanlan.zhihu.com/p/68029391?utm_source=qq&utm_medium=social&utm_oi=566394839504048128)


### 推荐系统的RL挑战

几乎对所有的在线服务平台来说，推荐系统都是很关键的一部分。系统和用户之间的交互一般是这样的：系统推荐一个页面给用户，用户提供反馈，然后系统再推荐一个新的页面。

构建推荐系统的常用方式是根据损失函数评估可以使模型预测结果和即时用户响应之间差异最小化的模型。换句话说，这些模型没有明确考虑`用户的长期兴趣`。**但用户的兴趣会根据他看到的内容随着时间而变化，而推荐者的行为可能会显著影响这样的变化。**

从某种意义上讲，推荐行为其实是通过凸显特定物品并隐藏其他物品来引导用户兴趣的。因此，设计推荐策略会更好一点，比如基于强化学习（RL）的推荐策略——它可以考虑用户的长期兴趣。**但由于环境是与已经登陆的在线用户相对应的，因此 RL 框架在推荐系统设置中也遇到了一些挑战。**

首先，**驱动用户行为的兴趣点（奖励函数）一般是未知的**，但它对于 RL 算法的使用来说至关重要。在用于推荐系统的现有 RL 算法中，奖励函数一般是手动设计的（例如用 ±1 表示点击或不点击），这可能无法反映出用户对不同项目的偏好如何 (Zhao et al., 2018a; Zheng et al., 2018)。

其次，**无模型 RL 一般都需要和环境（在线用户）进行大量的交互才能学到良好的策略**。但这在推荐系统设置中是不切实际的。如果推荐看起来比较随机或者推荐结果不符合在线用户兴趣，她会很快放弃这一服务。

因此，**为了解决无模型方法样本复杂度大的问题，基于模型的 RL 方法更为可取。**近期有一些研究在相关但不相同的环境设置中训练机器人策略，结果表明基于模型的 RL 采样效率更高 (Nagabandi et al., 2017; Deisenroth et al., 2015; Clavera et al., 2018)。

基于模型的方法的优势在于可以`池化大量的离策略（off-policy）数据`，而且可以用这些数据学习良好的环境动态模型，而无模型方法只能用昂贵的在策略（on-policy）数据学习。**但之前基于模型的方法一般都是根据物理或高斯过程设计的，而不是根据用户行为的复杂序列定制的。**

### 解决方案

未完待续...










