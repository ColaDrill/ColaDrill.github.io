---
layout:     post
title:      强化学习总览
subtitle:   A Survey on Reinforcement Learning
date:       2018-12-02
author:     Jiayue Cai
header-img: img/post-bg-ai.jpg
catalog: true
tags:
    - Reinforcement Learning
---


>>Last updated on 2018-12-02... 

> [深度强化学习博客链接](https://coladrill.github.io/2018/10/21/%E6%B7%B1%E5%BA%A6%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E7%BB%BC%E8%BF%B0/)

强化学习本身是一个非常通用的人工智能范式，在直觉上让人觉得非常适合用来模拟各种时序决策任务，如语音、文本类任务。

当它和深度神经网络这种只要给我足够层和足够多的神经元，可以逼近任何函数的非线性函数近似模型结合在一起简直完美，无怪乎 DeepMind团队 经常号称`人工智能=深度学习+强化学习`。
	
### 定义

![](/img/post/20181202/1.png)
在强化学习中，有两个可以进行交互的对象：智能体和环境：
-  `智能体（agent）`可以感知外界环境的状态（state）和反馈的奖励（reward），并进行学习和决策。智能体的决策功能是指根据外界环境的状态来做出不同的动作（action），而学习功能是指根据外界环境的奖励来调整策略。
-  `环境（environment）`是智能体外部的所有事物，并受智能体动作的影响而改变其状态，并反馈给智能体相应的奖励。

#### 基本要素

- `状态s`是对环境的描述，可以是离散的或连续的，其状态空间为S；
- `动作a`是对智能体行为的描述，可以是离散的或连续的，其动作空间为A；
- `策略π(a|s)`是智能体根据环境状态s来决定下一步的动作a的函数；
- `状态转移概率 p(s′|s, a)`是在智能体根据当前状态 s 做出一个动作 a 之后，环境在下一个时刻转变为状态s′的概率；
- `即时奖励r(s, a, s′)`是一个标量函数，即智能体根据当前状态s做出动作a之后，环境会反馈给智能体一个奖励，这个奖励也经常和下一个时刻的状态s′ 有关。

#### 策略 

智能体的策略（policy）就是智能体如何根据环境状态s来决定下一步的动作a，通常可以分为下面两组：
- `确定性策略（Deterministic Policy）`是从状态空间到动作空间的映射函数π : S → A。
- `随机性策略（StochasticPolicy）`表示在给定环境状态时，智能体选择某个动作的概率分布。
![](/img/post/20181202/2.png)

通常情况下，强化学习一般使用随机性的策略。随机性的策略可以有很多优点:
- 在学习时可以通过引入一定随机性更好地探索环境。
- 二是使得策略更加地多样性。

> 比如在围棋中，确定性策略总是会在同一个位置上下棋，会导致你的策略很容易被对手预测。

#### 马尔可夫决策过程

Markov Decision Process，`MDP`

1、为了简单起见，我们将智能体与环境的交互看作是离散的时间序列。下图给出了智能体与环境的交互。
![](/img/post/20181202/3.png)

智能体从感知到的初始环境s0 开始，然后决定做一个相应的动作a<sub>0</sub>，环境相应地发生改变到新的状态s<sub>1</sub>，并反馈给智能体一个即时奖励r<sub>1</sub>，然后智能体又根据状态s<sub>1</sub>做一个动作a<sub>1</sub>，环境相应改变为s<sub>2</sub>，并反馈奖励r<sub>2</sub>。这样的交互可以一直进行下去。
![](/img/post/20181202/4.png)
其中r<sub>t</sub> = r(s<sub>t−1</sub>, a<sub>t−1</sub>, s<sub>t</sub>)是`第t时刻的即时奖励`。

智能体与环境的交互的过程可以看作是一个马尔可夫决策过程。

2、马尔可夫过程（Markov Process）是具有马尔可夫性的随机变量序列s<sub>0</sub>, s<sub>1</sub>, ... ，s<sub>t</sub> ∈ S，其下一个时刻的状态s<sub>t+1</sub>只取决于当前状态s<sub>t</sub>，
![](/img/post/20181202/5.png)
其中p(s<sub>t+1</sub>&#124;s<sub>t</sub>)称为状态转移概率
![](/img/post/20181202/6.png)

加入一个额外的变量：动作 a
![](/img/post/20181202/7.png)
其中p(s<sub>t+1</sub>&#124;s<sub>t</sub>, a<sub>t</sub>)为状态转移概率。

3、**最终**，给定`策略π(a&#124;s)`，马尔可夫决策过程的一个`轨迹（trajectory）`
![](/img/post/20181202/8.png)
的概率为
![](/img/post/20181202/9.png)
下图给出了马尔可夫决策过程的图模型表示
![](/img/post/20181202/10.png)

### RL的目标函数

#### 总回报

给定策略 π(a|s)，智能体和环境一次交互过程的轨迹 τ 所收到的累积奖励为总回报（return）。
![](/img/post/20181202/11.png)
- 假设环境中有一个或多个特殊的终止状态（terminal state），**当到达终止状态时，一个智能体和环境的交互过程就结束了。**
- 这一轮交互的过程称为一个`回合（episode）`或`试验（trial）`。
- 一般的强化学习任务（比如下棋、游戏）都属于这种`回合式的任务`。

如果环境中没有终止状态（比如终身学习的机器人），即T = ∞，称为`持续性强化学习`任务，其总回报也可能是无穷大。为了解决这个问题，我们可以引入一个`折扣率`来降低远期回报的权重。折扣回报（discounted return）定义为
![](/img/post/20181202/12.png)
其中γ ∈ [0, 1]是折扣率。当γ 接近于0时，智能体更在意`短期回报`；而当γ 接近于1时，`长期回报`变得更重要。

#### 目标函数

因为策略和状态转移都有一定的随机性，每次试验得到的轨迹是一个随机序列，其收获的总回报也不一样。**强化学习的目标是学习到一个策略π<sub>θ</sub>(a|s)来`最大化期望回报（expected return）`**，即希望智能体执行一系列的动作来获得尽可能多的平均回报。 
![](/img/post/20181202/13.png)
其中θ 为策略函数的参数。

### RL的值函数

为了评估一个策略π 的期望回报，我们定义两个值函数：状态值函数和状态-动作值函数。

#### 状态值函数

一个策略π的`期望回报`可以分解为
![](/img/post/20181202/14.png)
其中V<sup>π</sup>(s)称为状态值函数（state value function），表示从状态s开始，执行策略π得到的`期望总回报`
![](/img/post/20181202/15.png)
其中τ<sub>s<sub>0</sub></sub> 表示轨迹τ 的起始状态。

为了方便起见，我们用τ<sub>0:T</sub> 来表示从轨迹s<sub>0</sub>, a<sub>0</sub>, s<sub>1</sub>, ... , s<sub>T</sub>，用τ<sub>1:T</sub>表示轨迹s<sub>1</sub>, a<sub>1</sub>, ... , s<sub>T</sub>，因此有τ<sub>0:T</sub> = s<sub>0</sub>, a<sub>0</sub>, τ<sub>1:T</sub>。

根据马尔可夫性V<sup>π</sup>(s)可展开得到
![](/img/post/20181202/16.png)
**上式最后一行也称为`贝尔曼方程（Bellman equation）`**，表示当前状态的值函数可以通过下个状态的值函数来计算。

如果给定策略π(a&#124;s)，状态转移概率p(s′&#124;s, a)和奖励r(s, a, s′)，我们就可以通过迭代的方式来计算V<sup>π</sup>(s)。

**由于存在折扣率，迭代一定步数后，每个状态的值函数就会固定不变。**

#### 状态-动作值函数（Q函数）

贝尔曼方程中的第二个期望是指初始状态为s 并进行动作a，然后执行策略π 得到的期望总回报，称为状态-动作值函数（state-action value function）
![](/img/post/20181202/17.png)
状态-动作值函数也经常称为Q函数`（Q-function）`。

状态值函数V<sup>π</sup>(s)是Q函数Q<sup>π</sup>(s, a)关于动作a的期望：
![](/img/post/20181202/18.png)

结合上面两个公式，Q函数可以写为
![](/img/post/20181202/19.png)

#### 值函数的作用

值函数可以看作是对策略π的评估。如果在状态s，有一个动作a使得Q<sup>π</sup>(s, a) > V<sup>π</sup>(s)，说明执行动作a比当前的策略π(a&#124;s)要好，我们就可以调整参数使得策略π(a&#124;s)的概率增加。

### RL的分类

强化学习的算法非常多，大体上可以分为`基于值函数的方法`（包括动态规划、时序差分学习等）、`基于策略函数的方法`（包括策略梯度等）以及融合两者的方法。不同算法之间的关系如下图所示。
![](/img/post/20181202/20.png)

- 一般而言，基于值函数的方法策略更新时可能会导致值函数的改变比较大，对`收敛性`有一定影响，而基于策略函数的方法在策略更新时更加更平稳些。
- 但后者因为策略函数的`解空间`比较大，难以进行充分的采样，导致方差较大，并容易收敛到局部最优解。
- Actor-Critic算法通过`融合`两种方法，取长补短，有着更好的收敛性。 

### 基于值函数的学习方法

值函数是对策略π的评估，如果策略π有限（即状态数和动作数都有限）时，可以对所有的策略进行评估并选出最优策略π<sup>∗</sup>。
![](/img/post/20181202/21.png)
但这种方式在实践中很难实现。假设状态空间S 和动作空间A都是离散且有限的，策略空间为&#124;A&#124;<sup>&#124;S&#124;</sup>，往往也非常大。

一种可行的方式是通过迭代的方法不断优化策略，直到选出最优策略。

对于一个策略π(a&#124;s)，其Q函数为Q<sup>π</sup>(s, a)，我们可以设置一个新的策略π′(a&#124;s)
![](/img/post/20181202/22.png)
即π′(a|s)为一个确定性的策略，也可以直接写为
![](/img/post/20181202/23.png)
如果执行π'，会有
![](/img/post/20181202/24.png)

根据上式，我们可以通过下面方式来学习最优策略：**先随机初始化一个策略，计算该策略的值函数，并根据值函数来设置新的策略，然后一直反复迭代直到收敛。**

基于值函数的策略学习方法中最关键的是如何计算策略π 的值函数，一般有`动态规划`或`蒙特卡罗`两种计算方式。

#### 动态规划算法

从贝尔曼方程可知，如果知道马尔可夫决策过程的状态转移概率p(s′&#124;s, a)和奖励r(s, a, s′)，我们直接可以通过贝尔曼方程来迭代计算其值函数。这种`模型已知`的强化学习算法也称为`基于模型的强化学习（Model-Based ReinforcementLearning）算法`，这里的模型就是指马尔可夫决策过程。 

在已知模型时，可以通过动态规划的方法来计算。常用的方法主要有`策略迭代算法`和`值迭代算法`。

**1、策略迭代（Policy Iteration）**

每次迭代可以分为两步：
- `策略评估（policy evaluation）`：计算当前策略下，每个状态的值函数，即算法14.1中的3-6步。策略评估可以通过贝尔曼方程进行迭代计算V<sup>π</sup>(s)。如果状态数有限时，也可以通过直接求解Bellman方程来得到得到V<sup>π</sup>(s)。
- `策略改进（policy improvement）`：根据值函数来更新策略，即算法14.1中的7-8步。

![](/img/post/20181202/25.png)

> 策略迭代中的策略评估和策略改进是交替轮流进行，其中策略评估也是通过一个内部迭代来进行计算，其计算量比较大。

> 事实上，我们不需要每次计算出每次策略对应的精确的值函数，也就是说内部迭代不需要执行到完全收敛。

**2、值迭代（Value Iteration）**

值迭代方法将策略评估和策略改进两个过程合并，来直接计算出最优策略。

假设最优策略π<sup>∗</sup>对应的值函数称为最优值函数，那么最优状态值函数V<sup>∗</sup>(s)和最优状态-动作值函数Q<sup>∗</sup>(s, a)的关系为
![](/img/post/20181202/26.png)

根据贝尔曼方程可知，最优状态值函数V<sup>∗</sup>(s)和最优状态-动作值函数Q<sup>∗</sup>(s, a)也可以进行迭代计算。
![](/img/post/20181202/27.png)
![](/img/post/20181202/28.png)
这两个公式称为贝尔曼最优方程（Bellman Optimality Equation）。 

值迭代方法通过直接优化贝尔曼最优方程，迭代计算最优值函数。值迭代方法如算法14.2所示。
![](/img/post/20181202/29.png)

**3、策略迭代 VS 值迭代**

未完待续...





