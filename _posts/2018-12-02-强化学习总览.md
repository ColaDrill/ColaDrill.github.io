---
layout:     post
title:      强化学习总览
subtitle:   A Survey on Reinforcement Learning
date:       2018-12-02
author:     Jiayue Cai
header-img: img/post-bg-ai.jpg
catalog: true
tags:
    - Reinforcement Learning
---


>>Last updated on 2018-12-02... 
	
### 定义

![](/img/post/20181202/1.png)
在强化学习中，有两个可以进行交互的对象：智能体和环境：
-  `智能体（agent）`可以感知外界环境的状态（state）和反馈的奖励（reward），并进行学习和决策。智能体的决策功能是指根据外界环境的状态来做出不同的动作（action），而学习功能是指根据外界环境的奖励来调整策略。
-  `环境（environment）`是智能体外部的所有事物，并受智能体动作的影响而改变其状态，并反馈给智能体相应的奖励。

#### 基本要素

- `状态s`是对环境的描述，可以是离散的或连续的，其状态空间为S；
- `动作a`是对智能体行为的描述，可以是离散的或连续的，其动作空间为A；
- `策略π(a|s)`是智能体根据环境状态s来决定下一步的动作a的函数；
- `状态转移概率 p(s′|s, a)`是在智能体根据当前状态 s 做出一个动作 a 之后，环境在下一个时刻转变为状态s′的概率；
- `即时奖励r(s, a, s′)`是一个标量函数，即智能体根据当前状态s做出动作a之后，环境会反馈给智能体一个奖励，这个奖励也经常和下一个时刻的状态s′ 有关。

#### 策略 

智能体的策略（policy）就是智能体如何根据环境状态s来决定下一步的动作a，通常可以分为下面两组：
- `确定性策略（Deterministic Policy）`是从状态空间到动作空间的映射函数π : S → A。
- `随机性策略（StochasticPolicy）`表示在给定环境状态时，智能体选择某个动作的概率分布。
![](/img/post/20181202/2.png)

通常情况下，强化学习一般使用随机性的策略。随机性的策略可以有很多优点:
- 在学习时可以通过引入一定随机性更好地探索环境。
- 二是使得策略更加地多样性。

> 比如在围棋中，确定性策略总是会在同一个位置上下棋，会导致你的策略很容易被对手预测。

#### 马尔可夫决策过程

Markov Decision Process，`MDP`

1、为了简单起见，我们将智能体与环境的交互看作是离散的时间序列。下图给出了智能体与环境的交互。
![](/img/post/20181202/3.png)

智能体从感知到的初始环境s0 开始，然后决定做一个相应的动作a<sub>0</sub>，环境相应地发生改变到新的状态s<sub>1</sub>，并反馈给智能体一个即时奖励r<sub>1</sub>，然后智能体又根据状态s<sub>1</sub>做一个动作a<sub>1</sub>，环境相应改变为s<sub>2</sub>，并反馈奖励r<sub>2</sub>。这样的交互可以一直进行下去。
![](/img/post/20181202/4.png)
其中`r<sub>t<sub> = r(s<sub>t−1</sub>, a<sub>t−1</sub>, s<sub>t</sub>)`是第t时刻的即时奖励。

智能体与环境的交互的过程可以看作是一个马尔可夫决策过程。

2、马尔可夫过程（Markov Process）是具有马尔可夫性的随机变量序列s<sub>0</sub>, s<sub>1</sub>, ... ，s<sub>t</sub> ∈ S，其下一个时刻的状态s<sub>t+1</sub>只取决于当前状态s<sub>t</sub>，
![](/img/post/20181202/5.png)
其中p(s<sub>t+1</sub>|s<sub>t</sub>)称为状态转移概率
![](/img/post/20181202/6.png)

加入一个额外的变量：动作 a
![](/img/post/20181202/7.png)
其中p(st+1|st, at)为状态转移概率。

3、**最终**，给定`策略π(a|s)`，马尔可夫决策过程的一个`轨迹（trajectory）`
![](/img/post/20181202/8.png)
的概率为
![](/img/post/20181202/9.png)
下图给出了马尔可夫决策过程的图模型表示
![](/img/post/20181202/10.png)

