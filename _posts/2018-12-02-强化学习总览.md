---
layout:     post
title:      强化学习总览
subtitle:   A Survey on Reinforcement Learning
date:       2018-12-02
author:     Jiayue Cai
header-img: img/post-bg-ai.jpg
catalog: true
tags:
    - Reinforcement Learning
---


>>Last updated on 2018-12-02... 
	
强化学习本身是一个非常通用的人工智能范式，在直觉上让人觉得非常适合用来模拟各种时序决策任务，如语音、文本类任务。

当它和深度神经网络这种只要给我足够层和足够多的神经元，可以逼近任何函数的非线性函数近似模型结合在一起简直完美，无怪乎 DeepMind团队 经常号称`人工智能=深度学习+强化学习`。
	
### 定义

![](/img/post/20181202/1.png)
在强化学习中，有两个可以进行交互的对象：智能体和环境：
-  `智能体（agent）`可以感知外界环境的状态（state）和反馈的奖励（reward），并进行学习和决策。智能体的决策功能是指根据外界环境的状态来做出不同的动作（action），而学习功能是指根据外界环境的奖励来调整策略。
-  `环境（environment）`是智能体外部的所有事物，并受智能体动作的影响而改变其状态，并反馈给智能体相应的奖励。

#### 基本要素

- `状态s`是对环境的描述，可以是离散的或连续的，其状态空间为S；
- `动作a`是对智能体行为的描述，可以是离散的或连续的，其动作空间为A；
- `策略π(a|s)`是智能体根据环境状态s来决定下一步的动作a的函数；
- `状态转移概率 p(s′|s, a)`是在智能体根据当前状态 s 做出一个动作 a 之后，环境在下一个时刻转变为状态s′的概率；
- `即时奖励r(s, a, s′)`是一个标量函数，即智能体根据当前状态s做出动作a之后，环境会反馈给智能体一个奖励，这个奖励也经常和下一个时刻的状态s′ 有关。

#### 策略 

智能体的策略（policy）就是智能体如何根据环境状态s来决定下一步的动作a，通常可以分为下面两组：
- `确定性策略（Deterministic Policy）`是从状态空间到动作空间的映射函数π : S → A。
- `随机性策略（StochasticPolicy）`表示在给定环境状态时，智能体选择某个动作的概率分布。
![](/img/post/20181202/2.png)

通常情况下，强化学习一般使用随机性的策略。随机性的策略可以有很多优点:
- 在学习时可以通过引入一定随机性更好地探索环境。
- 二是使得策略更加地多样性。

> 比如在围棋中，确定性策略总是会在同一个位置上下棋，会导致你的策略很容易被对手预测。

#### 马尔可夫决策过程

Markov Decision Process，`MDP`

1、为了简单起见，我们将智能体与环境的交互看作是离散的时间序列。下图给出了智能体与环境的交互。
![](/img/post/20181202/3.png)

智能体从感知到的初始环境s0 开始，然后决定做一个相应的动作a<sub>0</sub>，环境相应地发生改变到新的状态s<sub>1</sub>，并反馈给智能体一个即时奖励r<sub>1</sub>，然后智能体又根据状态s<sub>1</sub>做一个动作a<sub>1</sub>，环境相应改变为s<sub>2</sub>，并反馈奖励r<sub>2</sub>。这样的交互可以一直进行下去。
![](/img/post/20181202/4.png)
其中`r<sub>t<sub> = r(s<sub>t−1</sub>, a<sub>t−1</sub>, s<sub>t</sub>)`是第t时刻的即时奖励。

智能体与环境的交互的过程可以看作是一个马尔可夫决策过程。

2、马尔可夫过程（Markov Process）是具有马尔可夫性的随机变量序列s<sub>0</sub>, s<sub>1</sub>, ... ，s<sub>t</sub> ∈ S，其下一个时刻的状态s<sub>t+1</sub>只取决于当前状态s<sub>t</sub>，
![](/img/post/20181202/5.png)
其中p(s<sub>t+1</sub>|s<sub>t</sub>)称为状态转移概率
![](/img/post/20181202/6.png)

加入一个额外的变量：动作 a
![](/img/post/20181202/7.png)
其中p(st+1|st, at)为状态转移概率。

3、**最终**，给定`策略π(a|s)`，马尔可夫决策过程的一个`轨迹（trajectory）`
![](/img/post/20181202/8.png)
的概率为
![](/img/post/20181202/9.png)
下图给出了马尔可夫决策过程的图模型表示
![](/img/post/20181202/10.png)

### 强化学习的目标函数

#### 总回报

给定策略 π(a|s)，智能体和环境一次交互过程的轨迹 τ 所收到的累积奖励为总回报（return）。
![](/img/post/20181202/11.png)
- 假设环境中有一个或多个特殊的终止状态（terminal state），**当到达终止状态时，一个智能体和环境的交互过程就结束了。**
- 这一轮交互的过程称为一个`回合（episode）`或`试验（trial）`。
- 一般的强化学习任务（比如下棋、游戏）都属于这种`回合式的任务`。

如果环境中没有终止状态（比如终身学习的机器人），即T = ∞，称为`持续性强化学习`任务，其总回报也可能是无穷大。为了解决这个问题，我们可以引入一个`折扣率`来降低远期回报的权重。折扣回报（discounted return）定义为
![](/img/post/20181202/12.png)
