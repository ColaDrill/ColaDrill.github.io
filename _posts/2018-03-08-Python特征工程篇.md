---
layout:     post
title:      Python特征工程篇
subtitle:   For Data Mining
date:       2018-03-08
author:     Jiayue Cai
header-img: img/post-bg-debug.png
catalog: true
tags:
    - Python
    - Feature Engineering
---


>Last updated on 2018-10-02... 

- [python语句](http://www.runoob.com/python/python-tutorial.html)
- [pandas常用函数](https://www.jianshu.com/p/6eb5499cd07d)、[pandas文档](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.html)
- [pygal通用画图](http://pygal.org/en/stable/documentation/types/maps/pygal_maps_world.html)
- [networkx社会网络图](https://networkx.github.io/documentation/networkx-2.1/#)
- [pyecharts地理地图](http://pyecharts.org/#/zh-cn/)、[pyecharts示例](http://pyecharts.herokuapp.com/geo)

文字总结版见我的另一篇博客[《特征工程（文字总结版）》](https://coladrill.github.io/2018/10/02/%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B-%E6%96%87%E5%AD%97%E6%80%BB%E7%BB%93%E7%89%88/)。

### 常用头文件 
```python
import numpy as np               #数学
import pandas as pd              #表格
import matplotlib.pyplot as plt  #图形
%matplotlib inline
```
### 特征工程（简单）

#### 数据选取

```python
df = pd.read_csv("data/train.csv")
df.describe()                    #各项统计
df.info()                        #看总数及数值类型 

df15 = df[df.iyear >= 2015]        #选取满足的行
df15 = df15[['eventid','region']]  #选取需要的列

data = pd.concat([train,predict])  #上下拼接
data = pd.merge(data,ad_feature,on='aid',how='left') #键值左连接

max(df.iyear) #返回最大值
min(df.iyear) #返回最小值

df['region'].value_counts()               #返回地区-地区出现次数，默认降序，升序为ascending=True
df['region'].value_counts(normalize=True) #返回地区-频率，默认降序

df2 = df.groupby(by=['region'])['eventid'].count()      #分地区记录发生的事件数
count = pd.DataFrame({"region":df2.index,"counts":df2})  #第一列地区，第二列该地区发生的事件数

df2 = df.groupby(by=['weapontype'])['nkill'].sum() #返回武器-该武器杀人总数
```

#### 缺失值处理

对于竞赛而言最好不要直接删除，最好另作`特殊编码`，或者想办法最大程度保留缺失值所带来的`信息`。
- 比如首先统计样本的缺失值数量，作为新的特征。
- 然后再将缺失数量做一个排序，如果发现3份数据（train、test、unlabeled）都呈阶梯状，于是就可以根据缺失数量将数据划分为若干部分，作为新的特征。

```python
df = df.drop(['PassengerId','Name','Ticket','Cabin'], axis=1)  #对于大量缺失数据的列可直接删除
df = df.dropna()                                               #删除含有NaN数据的行
df = df.fillna('-1')                                           #全部直接人工赋值

df['nkill'].fillna(0, inplace = True)                          #单列直接人工赋值
df['Embarked'] = df['Embarked'].fillna('S')                    #离散值填充众数  
median_age = train['Age'].median()                             #连续值填充中位数（或者平均值）
df['Age'] = df['Age'].fillna(median_age)
```	

#### 数据格式转换

其实就相当于简易的编码，需根据分类器的特性来，比如说树模型在求解分裂点的时候只考虑排序分位点。
GBDT更适合连续特征：在树分裂的时候，选择连续特征等于选择了一个特征；而选择离散特征时却等于选择了一个特征的某一维。
对于离散特征，要特征统计后分桶，将特征维度压缩到合理范围。（其实这出于效率，会降低精度，但一定程度上抑制了过拟合）

```python
df.loc[ (df.Sex == 'male'), 'Sex' ] = 0    #令男为0
df.loc[ (df.Sex == 'female'), 'Sex' ] = 1  #令女为1
df['Sex'] = df['Sex'].map( {'male': 0, 'female': 1} ).astype(int) #这种写法更好
```

#### 特征合并

```python
#1、称谓
df['Title'] = df['Name'].str.extract('([A-Za-z]+)\.', expand=False)
df['Title'] = df['Title'].fillna('NoTitle')
df['Title'] = df['Title'].replace(['Lady', 'Countess','Capt', 'Col','Don', 'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona'], 'Rare')
df['Title'] = df['Title'].replace(['Mlle','Ms'], 'Miss')
df['Title'] = df['Title'].replace('Mme', 'Mrs')

#2、亲戚数量与子女数量
df['Companions'] = df['Parch'] + df['SibSp']
to_be_dropped.extend(['Parch', 'SibSp'])
```

#### 指标筛选（From数学系）

指标筛选分为显著性分析和因子分析两步。
- 显著性分析：通过T检验方法分析正负样本，找出能够明显区分样本的指标。
- 因子分析：在上面的基础上对筛选出来的指标计算主成分特征值，从中找出特征值大的指标作为最终评价指标。

### 特征工程（进阶）

#### 原始特征（取topK）

```python
#代码未补
```

#### 排序特征

将原始数值特征进行升序排序，将得到的rank作为新的特征。比如特征是15，10，2，100 ，排序后的新特征就是3，2，1，4
排序特征后通常用SVM来做，训练前需要对特征做归一化

```python
import pandas as pd

feature_type = pd.read_csv('../data/features_type.csv')
numeric_feature = list(feature_type[feature_type.type=='numeric'].feature)

#rank特征的命名：在原始特征前加'r',如'x1'的rank特征为'rx1'

#三份数据集分别排序，使用的时候需要归一化。
#更合理的做法是merge到一起排序，这个我们也试过，效果差不多，因为数据分布相对比较一致。

test = pd.read_csv('../data/test_x.csv')[['uid']+numeric_feature]
test_rank = pd.DataFrame(test.uid,columns=['uid'])
for feature in numeric_feature:
    test_rank['r'+feature] = test[feature].rank(method='max')
test_rank.to_csv('../data/test_x_rank.csv',index=None)
```

#### 离散特征（区间划分）

`等值划分`（按照值域均分）

```python
df.loc[ df['Age'] <= 16, 'Age'] = 1
df.loc[ (df['Age'] > 16) & (df['Age'] <= 32), 'Age'] = 2
df.loc[ (df['Age'] > 32) & (df['Age'] <= 48), 'Age'] = 3
df.loc[ (df['Age'] > 48) & (df['Age'] <= 64), 'Age'] = 4
df.loc[ df['Age'] > 64, 'Age'] = 5
df['Age'] = df['Age'].astype(int)
```

`等量划分`（按照样本数均分）

```python
train = pd.read_csv("../data/train_x_rank.csv")
train_x = train.drop(['uid'],axis=1)

train_x[train_x<1500] = 1
train_x[(train_x>=1500)&(train_x<3000)] = 2
train_x[(train_x>=3000)&(train_x<4500)] = 3
...
train_x[train_x>=13500] = 10

#离散特征的命名：在原始特征前加'd',如'x1'的离散特征为'dx1'
rename_dict = {s:'d'+s[1:] for s in train_x.columns.tolist()}
train_x = train_x.rename(columns=rename_dict)
train_x['uid'] = train.uid
train_x.to_csv('../data/train_x_discretization.csv',index=None)
```

#### 计数特征

计算每个样本离散特征1-10的数量，生成10个新的特征
- 以 uid 为 1 的样本为例，离散化后它的特征是5,3,1,3,3,3,2,4,3,2,5,3,2,3,2...2,2,2,2,2,2,2
- 可以进一步统计离散特征中 1~10 出现的次数n<sub>i</sub>(i=1,2,…,10)，即可得到一个 10 维计数特征
- 基于这 10 维特征训练了 xgboost 分类器，线上得分是 0.58 左右，说明这 10 维特征具有不错的判别性

```python
train_x = pd.read_csv('../data/train_x_discretization.csv')

train_x['n1'] = (train_x==1).sum(axis=1)
train_x['n2'] = (train_x==2).sum(axis=1)
train_x['n3'] = (train_x==3).sum(axis=1)
...
train_x['n10'] = (train_x==10).sum(axis=1)

train_x[['uid','n1','n2','n3','n4','n5','n6','n7','n8','n9','n10']].to_csv('../data/train_x_nd.csv',index=None)
```

#### 类别特征（连续编码为哑变量）

```python
from sklearn.preprocessing import OneHotEncoder,LabelEncoder
one_hot_feature=['LBS','age','carrier','consumptionAbility','education','gender','house','os','ct','marriageStatus']
for feature in one_hot_feature:   	          #LabelEncoder将各种标签分配一个可数的连续编号
	try:
		data[feature] = LabelEncoder().fit_transform(data[feature].apply(int))
	except:
		data[feature] = LabelEncoder().fit_transform(data[feature])
			
enc = OneHotEncoder()                         #one-hot表示（这一步须结合分类器特性）
for feature in one_hot_feature:
	enc.fit(data[feature].values.reshape(-1, 1))
	train_x=enc.transform(train[feature].values.reshape(-1, 1))
	test_x = enc.transform(test[feature].values.reshape(-1, 1))
```

#### 多值特征（top编码）

```python
from sklearn.preprocessing import LabelEncoder
vecc = vec['interest1'].value_counts()[:2000]  #代表出现数量前2000名的兴趣子片段，调大可提高精度
vecc_Fragment = vecc.index.tolist()
vecc_laber_encoder = LabelEncoder().fit_transform(vecc_Fragment)
vecci = dict(zip(vecc_Fragment,vecc_laber_encoder))
```	

#### 统计特征

```python
#1、长度
vector_feature=['interest1','interest2','interest5','kw1','kw2','topic1','topic2']
for feat in vector_feature:                    
	data['len_'+feat] = data[feat].apply(lambda x:0 if ((not x.strip()) or str(x)=='-1') else len(x.split(' ')))

#2、平均值
data['meanlen_interest'] = (data['len_interest1']+data['len_interest2']+data['len_interest5'])/3.0

#3、最大值
data['max_ct'] = data['ct'].apply(lambda x: 0 if ((not str(x).strip()) or str(x)=='-1') else max(int(i) for i in str(x).split(' ')))  
```

#### 转化率特征（命中率|CTR）

```python
num_ad = train['aid'].value_counts().sort_index()
num_ad_clicked = data_clicked['aid'].value_counts().sort_index()
ratio_ad_clicked = num_ad_clicked / num_ad
ratio_ad_clicked = pd.DataFrame({
	'aid': ratio_ad_clicked.index,
	'ratio_ad_clicked' : ratio_ad_clicked.values
})
data = pd.merge(data, ratio_ad_clicked, on=['aid'], how='left')
```

#### 交叉特征（特征组合）

人工：

- 将特征进行两两交叉x*y、 x^2+y^2、 1/x+1/y等等，在生成特征的同时计算与标签列的皮尔逊相关系数保留topK特征。

一般来说人工交叉特征效率很低，现有的[FM、FFM、DNN模型](https://mp.weixin.qq.com/s?__biz=MzIwMTc4ODE0Mw==&mid=2247490455&idx=2&sn=591ba0ca8dd660ce956ec737a6a277e4&chksm=96e9c417a19e4d0157c77446a727af0bdf27005ae8acfb7f89300f3ec6d56f3493e6984a5d01&mpshare=1&scene=23&srcid=071960wJ6bOpflL1O9ypGbqx#rd)则在一定程度上做到了自动特征交叉。

- FM:  引入了交叉特征，增加了模型的非线性
- FFM: 把n个特征归属到f个field里，得到nf个隐向量的二次项
- DNN: 能够学习出高阶非线性特征；容易扩充其他类别的特征，比如在特征拥有图片，文字类特征的时候

#### 归一化

对于那些目标变量为输入特征的`光滑函数的模型`，如`线性回归`、`逻辑回归`等，其对输入特征的大小很敏感，`有必要`对输入进行`归一化`。

而对于那些`基于树的模型`，如`随机森林`、`梯度提升树`等，其对输入特征的大小不敏感，故`不需要归一化`。

```python
#将新加入的特征归一化
from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()
scaler.fit(train[['ratio_ad_clicked', 'num_ad_push2user']].values)
train_x = scaler.transform(train[['ratio_ad_clicked', 'num_ad_push2user']].values)
```	

#### 特征选择

特征选择是特征工程里的一个重要问题，其目标是寻找最优特征子集。（具体看下一篇博客）