---
layout:     post
title:      Python特征工程篇
subtitle:   For Data Mining
date:       2018-03-08
author:     Jiayue Cai
header-img: img/post-bg-debug.png
catalog: true
tags:
    - Python
    - Feature Engineering
---


>Last updated on 2018-9-20... 

- [python语句](http://www.runoob.com/python/python-tutorial.html)
- [pandas常用函数](https://www.jianshu.com/p/6eb5499cd07d)、[pandas文档](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.html)
- [pygal通用画图](http://pygal.org/en/stable/documentation/types/maps/pygal_maps_world.html)
- [networkx社会网络图](https://networkx.github.io/documentation/networkx-2.1/#)
- [pyecharts地理地图](http://pyecharts.org/#/zh-cn/)、[pyecharts示例](http://pyecharts.herokuapp.com/geo)

### 常用头文件 
```python
import numpy as np               #数学
import pandas as pd              #表格
import matplotlib.pyplot as plt  #图形
%matplotlib inline
```
### 特征工程（简单）

#### 数据选取

```python
df = pd.read_csv("data/train.csv")
df.describe()                    #各项统计
df.info()                        #看总数及数值类型 

df15 = df[df.iyear >= 2015]        #选取满足的行
df15 = df15[['eventid','region']]  #选取需要的列

data = pd.concat([train,predict])  #上下拼接
data = pd.merge(data,ad_feature,on='aid',how='left') #键值左连接

max(df.iyear) #返回最大值
min(df.iyear) #返回最小值

df['region'].value_counts()               #返回地区-地区出现次数，默认降序，升序为ascending=True
df['region'].value_counts(normalize=True) #返回地区-频率，默认降序

df2 = df.groupby(by=['region'])['eventid'].count()      #分地区记录发生的事件数
count = pd.DataFrame({"region":df2.index,"counts":df2})  #第一列地区，第二列该地区发生的事件数

df2 = df.groupby(by=['weapontype'])['nkill'].sum() #返回武器-该武器杀人总数
```

#### 缺失值处理

对于竞赛而言最好不要直接删除，最好另作`特殊编码`，或者想办法`最大程度保留`缺失值所带来的`信息`。
- 比如首先统计样本的缺失值数量，作为新的特征。
- 然后再将缺失数量做一个排序，如果发现3份数据（train、test、unlabeled）都呈阶梯状，于是就可以根据缺失数量将数据划分为若干部分，作为新的特征。

```python
df = df.drop(['PassengerId','Name','Ticket','Cabin'], axis=1)  #对于大量缺失数据的列可直接删除
df = df.dropna()                                               #删除含有NaN数据的行
df = df.fillna('-1')                                           #全部直接人工赋值

df['nkill'].fillna(0, inplace = True)                          #单列直接人工赋值
df['Embarked'] = df['Embarked'].fillna('S')                    #离散值填充众数  
median_age = train['Age'].median()                             #连续值填充中位数（或者平均值）
df['Age'] = df['Age'].fillna(median_age)
```	

#### 数据格式转换

其实就相当于简易的编码，需根据分类器的特性来，比如说树模型在求解分裂点的时候只考虑排序分位点。
GBDT更适合连续特征：在树分裂的时候，选择连续特征等于选择了一个特征；而选择离散特征时却等于选择了一个特征的某一维。
对于离散特征，要特征统计后分桶，将特征维度压缩到合理范围。（其实这出于效率，会降低精度，但一定程度上抑制了过拟合）

```python
df.loc[ (df.Sex == 'male'), 'Sex' ] = 0    #令男为0
df.loc[ (df.Sex == 'female'), 'Sex' ] = 1  #令女为1
df['Sex'] = df['Sex'].map( {'male': 0, 'female': 1} ).astype(int) #这种写法更好
```

#### 特征合并

```python
#1、称谓
df['Title'] = df['Name'].str.extract('([A-Za-z]+)\.', expand=False)
df['Title'] = df['Title'].fillna('NoTitle')
df['Title'] = df['Title'].replace(['Lady', 'Countess','Capt', 'Col','Don', 'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona'], 'Rare')
df['Title'] = df['Title'].replace(['Mlle','Ms'], 'Miss')
df['Title'] = df['Title'].replace('Mme', 'Mrs')

#2、亲戚数量与子女数量
df['Companions'] = df['Parch'] + df['SibSp']
to_be_dropped.extend(['Parch', 'SibSp'])
```

#### 特征选择

特征选择是特征工程里的一个重要问题，其目标是寻找最优特征子集，具体可以参照[这篇文章](https://zhuanlan.zhihu.com/p/32749489)。
- Filter：过滤法，按照发散性或者相关性对各个特征进行评分，设定阈值或者待选择阈值的个数，选择特征。
- Wrapper：包装法，根据目标函数（通常是预测效果评分），每次选择若干特征，或者排除若干特征。
- Embedded：嵌入法，先使用某些机器学习的算法和模型进行训练，得到各个特征的权值系数，根据系数从大到小排序选择特征。类似于Filter方法，但是是通过训练来确定特征的优劣。

#### 指标筛选（From数学系）

指标筛选分为显著性分析和因子分析两步。
- 显著性分析：通过T检验方法分析正负样本，找出能够明显区分样本的指标。
- 因子分析：在上面的基础上对筛选出来的指标计算主成分特征值，从中找出特征值大的指标作为最终评价指标。

### 特征工程（进阶）

#### 单值连续特征(连续编码)

```python
from sklearn.preprocessing import OneHotEncoder,LabelEncoder
one_hot_feature=['LBS','age','carrier','consumptionAbility','education','gender','house','os','ct','marriageStatus']
for feature in one_hot_feature:   	          #LabelEncoder将各种标签分配一个可数的连续编号
	try:
		data[feature] = LabelEncoder().fit_transform(data[feature].apply(int))
	except:
		data[feature] = LabelEncoder().fit_transform(data[feature])
			
enc = OneHotEncoder()                         #one-hot表示（这一步须结合分类器特性）
for feature in one_hot_feature:
	enc.fit(data[feature].values.reshape(-1, 1))
	train_a=enc.transform(train[feature].values.reshape(-1, 1))
	test_a = enc.transform(test[feature].values.reshape(-1, 1))
	train_x= sparse.hstack((train_x, train_a))
	test_x = sparse.hstack((test_x, test_a))
```

#### 多值特征（top编码）

```python
from sklearn.preprocessing import LabelEncoder
vecc = vec['interest1'].value_counts()[:2000]  #代表出现数量前2000名的兴趣子片段，调大可提高精度
vecc_Fragment = vecc.index.tolist()
vecc_laber_encoder = LabelEncoder().fit_transform(vecc_Fragment)
vecci = dict(zip(vecc_Fragment,vecc_laber_encoder))
```	

#### 统计特征

```python
#1、长度
vector_feature=['interest1','interest2','interest5','kw1','kw2','topic1','topic2']
for feat in vector_feature:                    
	data['len_'+feat] = data[feat].apply(lambda x:0 if ((not x.strip()) or str(x)=='-1') else len(x.split(' ')))

#2、平均值
data['meanlen_interest'] = (data['len_interest1']+data['len_interest2']+data['len_interest5'])/3.0

#3、最大值
data['max_ct'] = data['ct'].apply(lambda x: 0 if ((not str(x).strip()) or str(x)=='-1') else max(int(i) for i in str(x).split(' ')))  
	
#4、分层
df.loc[ df['Age'] <= 16, 'Age'] = 0
df.loc[ (df['Age'] > 16) & (df['Age'] <= 32), 'Age'] = 1
df.loc[ (df['Age'] > 32) & (df['Age'] <= 48), 'Age'] = 2
df.loc[ (df['Age'] > 48) & (df['Age'] <= 64), 'Age'] = 3
df.loc[ df['Age'] > 64, 'Age'] = 4
df['Age'] = df['Age'].astype(int)
```

#### 转化率特征（命中率）

```python
num_ad = train['aid'].value_counts().sort_index()
num_ad_clicked = data_clicked['aid'].value_counts().sort_index()
ratio_ad_clicked = num_ad_clicked / num_ad
ratio_ad_clicked = pd.DataFrame({
	'aid': ratio_ad_clicked.index,
	'ratio_ad_clicked' : ratio_ad_clicked.values
})
data = pd.merge(data, ratio_ad_clicked, on=['aid'], how='left')
```

#### 交叉特征（特征组合）

一般来说人工交叉特征效率很低，现有的[FM、FFM、DNN模型](https://mp.weixin.qq.com/s?__biz=MzIwMTc4ODE0Mw==&mid=2247490455&idx=2&sn=591ba0ca8dd660ce956ec737a6a277e4&chksm=96e9c417a19e4d0157c77446a727af0bdf27005ae8acfb7f89300f3ec6d56f3493e6984a5d01&mpshare=1&scene=23&srcid=071960wJ6bOpflL1O9ypGbqx#rd)则在一定程度上做到了自动特征交叉。
- FM:  引入了交叉特征，增加了模型的非线性
- FFM: 把n个特征归属到f个field里，得到nf个隐向量的二次项
- DNN: 能够学习出高阶非线性特征；容易扩充其他类别的特征，比如在特征拥有图片，文字类特征的时候

#### 归一化

```python
#将新加入的特征归一化
from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()
scaler.fit(train[['ratio_ad_clicked', 'num_ad_push2user']].values)
train_x = scaler.transform(train[['ratio_ad_clicked', 'num_ad_push2user']].values)
```		
	