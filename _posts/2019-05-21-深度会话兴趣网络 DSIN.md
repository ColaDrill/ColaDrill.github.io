---
layout:     post
title:      深度会话兴趣网络 DSIN
subtitle:   序列切分、兴趣提取
date:       2019-05-21
author:     Jiayue Cai
header-img: img/post-bg-alibaba.jpg
catalog: true
tags:
    - Recommending System
---


> Last updated on 2019-6-2...

- [《匹配&推荐技术》](https://coladrill.github.io/2018/08/06/%E5%8C%B9%E9%85%8D&%E6%8E%A8%E8%8D%90%E6%8A%80%E6%9C%AF/)
- [《深度兴趣演化网络 DIEN》](https://coladrill.github.io/2019/02/04/%E6%B7%B1%E5%BA%A6%E5%85%B4%E8%B6%A3%E6%BC%94%E5%8C%96%E7%BD%91%E7%BB%9C-DIEN/)

> [论文链接](https://arxiv.org/pdf/1905.06482.pdf)、[主要参考链接](https://mp.weixin.qq.com/s/mSv_FQPBmvQDE4jlwjl4lA)

> [阿里首次将Transformer用于淘宝电商推荐](https://mp.weixin.qq.com/s?__biz=MzU1NDA4NjU2MA==&mid=2247496660&idx=1&sn=cb2af41f41fbdc591304156b0940f0fc&chksm=fbea4a1bcc9dc30d5e9620fa70de17941c007b457568c71fcca3d1ce5f2c3e00e008ad7f23d0&mpshare=1&scene=23&srcid=0601BpmadhpUmJe0hP7Iqei7#rd)、[细讲 Attention Is All You Need](https://mp.weixin.qq.com/s/RLxWevVWHXgX-UcoxDS70w)


### 场景 

通常，在每个会话中的用户行为是相近的，而在不同会话之间差别是很大的：
![](/img/post/20190521/1.png)

### DSIN模型

Base Model就是一个全连接神经网络，其输入的特征的主要分为三部分——用户特征、待推荐物品特征、用户历史行为序列特征。
- `用户特征`如性别、城市、用户ID等
- `待推荐物品特征`：如商家ID、品牌ID等
- `用户历史行为序列特征`：如用户最近点击的物品ID序列等

这些特征会通过Embedding层转换为对应的embedding，拼接后输入到多层全连接中，并使用logloss指导模型的训练。

![](/img/post/20190521/2.png)
DSIN在全连接层之前，分成了两部分：
- 左边：将用户特征和物品特征转换对应的向量表示，这部分主要是一个embedding层
- 右边：对用户行为序列进行处理，从下到上分为四层：
	- `序列切分层`session division layer
	- `会话兴趣抽取层`session interest extractor layer
	- `会话间兴趣交互层`session interest interacting layer
	- `会话兴趣激活层`session interest acti- vating layer

![](/img/post/20190521/3.png)

#### 序列切分层 session division layer

![](/img/post/20190521/4.png)

这一层将用户的行文进行切分，首先将用户的点击行为按照时间排序，判断每两个行为之间的时间间隔，前后的时间间隔大于30min就进行切分。

切分后，我们可以将用户的行为序列S转换成会话序列Q。

例如第k个会话表示为Q<sub>k</sub>=[b<sub>1</sub>;b<sub>2</sub>;...;b<sub>i</sub>;...;b<sub>T</sub>]。

其中，T是会话的长度，bi是会话中第i个行为，是一个d维的embedding向量。所以Qk是T * d的。而Q，则是K * T * d的。而Q，则是K。

#### 会话兴趣抽取层 session interest extractor layer

这里对每个session，使用transformer对每个会话的行为进行处理。

> [阿里首次将Transformer用于淘宝电商推荐](https://mp.weixin.qq.com/s?__biz=MzU1NDA4NjU2MA==&mid=2247496660&idx=1&sn=cb2af41f41fbdc591304156b0940f0fc&chksm=fbea4a1bcc9dc30d5e9620fa70de17941c007b457568c71fcca3d1ce5f2c3e00e008ad7f23d0&mpshare=1&scene=23&srcid=0601BpmadhpUmJe0hP7Iqei7#rd)、[细讲 Attention Is All You Need](https://mp.weixin.qq.com/s/RLxWevVWHXgX-UcoxDS70w)

在Transformer中，对输入的序列会进行Positional Encoding。Positional Encoding对序列中每个物品，以及每个物品对应的Embedding的每个位置，进行了处理，如下：
![](/img/post/20190521/5.png)

但在我们这里不一样了，我们同时会输入多个会话序列，所以还需要对每个会话添加一个Positional Encoding。

在DSIN中，这种对位置的处理，称为`Bias Encoding`，它分为三块：
![](/img/post/20190521/6.png)

BE是K * T * d的，和Q的形状一样。BE(k,t,c)是第k个session中，第t个物品的嵌入向量的第c个位置的偏置项。

也就是说，每个会话、会话中的每个物品有偏置项，每个物品对应的embedding的每个位置，都加入了偏置项。

所以加入偏置项后，Q变为：
![](/img/post/20190521/7.png)

随后，是对每个会话中的序列通过`Transformer`进行处理：
![](/img/post/20190521/8.png)

这里的过程和Transformer的Encoding的block处理是一样的。经过Transformer处理之后，每个Session是得到的结果仍然是T * d。

随后，我们经过一个avg pooling操作，将每个session兴趣转换成一个d维向量。
![](/img/post/20190521/9.png)

其中I<sub>k</sub>就代表第k个session对应的兴趣向量。

#### 会话间兴趣交互层 session interest interacting layer

用户的会话兴趣，是有序列关系在里面的，这种关系，我们通过一个`双向LSTM`(bi-LSTM)来处理：
![](/img/post/20190521/10.png)

每个时刻的hidden state计算如下:
![](/img/post/20190521/11.png)

相加的两项分别是前向传播和反向传播对应的t时刻的hidden state。这里得到的隐藏层状态Ht，我们可以认为是混合了上下文信息的会话兴趣。

#### 会话兴趣激活层 session interest acti- vating layer

用户的会话兴趣与目标物品越相近，那么应该赋予更大的权重，这里使用`注意力机制`来刻画这种相关性：
![](/img/post/20190521/12.png)
![](/img/post/20190521/13.png)

这里X<sup>I</sup>是带推荐物品向量。

同样，混合了上下文信息的会话兴趣，也进行同样的处理：
![](/img/post/20190521/14.png)
![](/img/post/20190521/15.png)

后面的话，就是把四部分的向量：用户特征向量、待推荐物品向量、会话兴趣加权向量UI、带上下文信息的会话兴趣加权向量UH进行横向拼接，输入到全连接层中，得到输出。

### 实验结果

本次使用了两个数据集进行了实验，分别是阿里妈妈的`广告数据集`和阿里巴巴的`电商推荐数据集`。

对比模型有：YoutubeNet、Wide & Deep、DIN 、DIN-RNN(这个和DIN很像，在原始的DIN中，用户的行为序列没有使用RNN进行处理，而DIN-RNN使用bi-LSTM对用户的历史行为序列进行处理)、DIEN。

评价指标是AUC,结果如下：
![](/img/post/20190521/16.png)












