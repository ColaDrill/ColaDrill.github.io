---
layout:     post
title:      Python & 初级特征工程
subtitle:   For Data Mining
date:       2018-03-08
author:     Jiayue Cai
header-img: img/post-bg-debug.png
catalog: true
tags:
    - Python
    - Feature Engineering
---


>Last updated on 2018-9-3... 

### 常用头文件 
```python
	import numpy as np               #数学
	import pandas as pd              #表格
	from pandas import Series,DataFrame
    import matplotlib.pyplot as plt  #图形
    %matplotlib inline
```
### 特征工程（简单）
```python
	df = pd.read_csv("data/train.csv")
	df.describe()                    #各项统计
	df.info()                        #看总数及数值类型 
```
#### 缺失值处理

对于竞赛而言最好不要直接删除，最好另作特殊编码
```python
	df = df.drop(['PassengerId','Name','Ticket','Cabin'], axis=1)  #对于大量缺失数据的列可直接删除
	df = df.dropna()                                               #删除含有NaN数据的行
	df['Embarked'] = df['Embarked'].fillna('S')                    #离散值填充众数  
	median_age = train['Age'].median()                             #连续值填充中位数（或者平均值）
	df['Age'] = df['Age'].fillna(median_age)
```	
#### 数据格式转换

其实就相当于简易的编码，需根据分类器的特性来，比如说树模型在求解分裂点的时候只考虑排序分位点。
GBDT更适合连续特征：在树分裂的时候，选择连续特征等于选择了一个特征；而选择离散特征时却等于选择了一个特征的某一维。
对于离散特征，要特征统计后分桶，将特征维度压缩到合理范围。（其实这出于效率，会降低精度，但一定程度上抑制了过拟合）
```python
	df.loc[ (df.Sex == 'male'), 'Sex' ] = 0    #令男为0
	df.loc[ (df.Sex == 'female'), 'Sex' ] = 1  #令女为1
	df['Sex'] = df['Sex'].map( {'male': 0, 'female': 1} ).astype(int) #这种写法更好
```
#### 特征合并
```python
	#1、称谓
	df['Title'] = df['Name'].str.extract('([A-Za-z]+)\.', expand=False)
    df['Title'] = df['Title'].fillna('NoTitle')
    df['Title'] = df['Title'].replace(['Lady', 'Countess','Capt', 'Col','Don', 'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona'], 'Rare')
    df['Title'] = df['Title'].replace(['Mlle','Ms'], 'Miss')
	df['Title'] = df['Title'].replace('Mme', 'Mrs')
	
	#2、亲戚数量与子女数量
    df['Companions'] = df['Parch'] + df['SibSp']
    to_be_dropped.extend(['Parch', 'SibSp'])
```
#### 特征选择

特征选择是特征工程里的一个重要问题，其目标是寻找最优特征子集，具体可以参照[这篇文章](https://zhuanlan.zhihu.com/p/32749489)。
- Filter：过滤法，按照发散性或者相关性对各个特征进行评分，设定阈值或者待选择阈值的个数，选择特征。
- Wrapper：包装法，根据目标函数（通常是预测效果评分），每次选择若干特征，或者排除若干特征。
- Embedded：嵌入法，先使用某些机器学习的算法和模型进行训练，得到各个特征的权值系数，根据系数从大到小排序选择特征。类似于Filter方法，但是是通过训练来确定特征的优劣。

### 特征工程（进阶）

#### 单值连续特征(连续编码)
```python
	from sklearn.preprocessing import OneHotEncoder,LabelEncoder
	one_hot_feature=['LBS','age','carrier','consumptionAbility','education','gender','house','os','ct','marriageStatus']
	for feature in one_hot_feature:   	          #LabelEncoder将各种标签分配一个可数的连续编号
		try:
			data[feature] = LabelEncoder().fit_transform(data[feature].apply(int))
		except:
			data[feature] = LabelEncoder().fit_transform(data[feature])
			
	enc = OneHotEncoder()                         #one-hot表示（这一步须结合分类器特性）
	for feature in one_hot_feature:
		enc.fit(data[feature].values.reshape(-1, 1))
		train_a=enc.transform(train[feature].values.reshape(-1, 1))
		test_a = enc.transform(test[feature].values.reshape(-1, 1))
		train_x= sparse.hstack((train_x, train_a))
		test_x = sparse.hstack((test_x, test_a))
```
#### 多值特征（top编码）
```python
	from sklearn.preprocessing import LabelEncoder
	vecc = vec['interest1'].value_counts()[:2000]  #代表出现数量前2000名的兴趣子片段，调大可提高精度
	vecc_Fragment = vecc.index.tolist()
	vecc_laber_encoder = LabelEncoder().fit_transform(vecc_Fragment)
	vecci = dict(zip(vecc_Fragment,vecc_laber_encoder))
```	
#### 统计特征
```python
	#1、长度
	vector_feature=['interest1','interest2','interest5','kw1','kw2','topic1','topic2']
	for feat in vector_feature:                    
		data['len_'+feat] = data[feat].apply(lambda x:0 if ((not x.strip()) or str(x)=='-1') else len(x.split(' ')))
		
	#2、平均值
	data['meanlen_interest'] = (data['len_interest1']+data['len_interest2']+data['len_interest5'])/3.0
	
    #3、最大值
	data['max_ct'] = data['ct'].apply(lambda x: 0 if ((not str(x).strip()) or str(x)=='-1') else max(int(i) for i in str(x).split(' ')))  
	
	#4、分层
	df.loc[ df['Age'] <= 16, 'Age'] = 0
    df.loc[ (df['Age'] > 16) & (df['Age'] <= 32), 'Age'] = 1
    df.loc[ (df['Age'] > 32) & (df['Age'] <= 48), 'Age'] = 2
    df.loc[ (df['Age'] > 48) & (df['Age'] <= 64), 'Age'] = 3
    df.loc[ df['Age'] > 64, 'Age'] = 4
    df['Age'] = df['Age'].astype(int)
```
#### 转化率特征（命中率）
```python
	num_ad = train['aid'].value_counts().sort_index()
	num_ad_clicked = data_clicked['aid'].value_counts().sort_index()
	ratio_ad_clicked = num_ad_clicked / num_ad
	ratio_ad_clicked = pd.DataFrame({
		'aid': ratio_ad_clicked.index,
		'ratio_ad_clicked' : ratio_ad_clicked.values
	})
	data = pd.merge(data, ratio_ad_clicked, on=['aid'], how='left')
```
#### 交叉特征（特征组合）

一般来说人工交叉特征效率很低，现有的[FM、FFM、DNN模型](https://mp.weixin.qq.com/s?__biz=MzIwMTc4ODE0Mw==&mid=2247490455&idx=2&sn=591ba0ca8dd660ce956ec737a6a277e4&chksm=96e9c417a19e4d0157c77446a727af0bdf27005ae8acfb7f89300f3ec6d56f3493e6984a5d01&mpshare=1&scene=23&srcid=071960wJ6bOpflL1O9ypGbqx#rd)则在一定程度上做到了自动特征交叉。
- FM:  引入了交叉特征，增加了模型的非线性
- FFM: 把n个特征归属到f个field里，得到nf个隐向量的二次项
- DNN: 能够学习出高阶非线性特征；容易扩充其他类别的特征，比如在特征拥有图片，文字类特征的时候

#### 归一化
```python
	#将新加入的特征归一化
	from sklearn.preprocessing import StandardScaler
	scaler = StandardScaler()
	scaler.fit(train[['ratio_ad_clicked', 'num_ad_push2user']].values)
	train_x = scaler.transform(train[['ratio_ad_clicked', 'num_ad_push2user']].values)
```		
### 模型调用（简单）
```python
	X_test = pd.read_csv("data/test.csv")
	from sklearn.svm import SVC, LinearSVC
	svc = SVC()
	svc.fit(df, dfs)
	Y_pred = svc.predict(X_test)
	svc.score(df, df.Survived)
```
### 模型调用（进阶）

#### 自定义迭代
```
	def LGB_predict(train_x,train_y,test_x,res):
		print("LGB test")
		clf = lgb.LGBMClassifier(
			boosting_type='gbdt', num_leaves=31, reg_alpha=0.0, reg_lambda=1,zero_as_missing=True,
			max_depth=-1, n_estimators=500, objective='binary',
			subsample=0.9, colsample_bytree=0.8, subsample_freq=1,
			learning_rate=0.1, min_child_weight=50, random_state=2018, n_jobs=100
		)
		clf.fit(train_x, train_y, eval_set=[(train_x, train_y)], eval_metric='auc',early_stopping_rounds=100)
		res['score'] = clf.predict_proba(test_x)[:,1]
		res['score'] = res['score'].apply(lambda x: float('%.6f' % x))
		res.to_csv('../data/submission.csv', index=False)
		os.system('zip baseline.zip ../data/submission.csv')
		return clf

	model=LGB_predict(train_x,train_y,test_x,res)
```
#### 使用管道
```python
    df = pd.read_csv('data/train.csv')
    y = df.author.values
    X = df.text.values

    df2 = pd.read_csv('data/test.csv')
    X2 = df2.text.values
	
	from sklearn.pipeline import Pipeline
    from sklearn.feature_extraction.text import CountVectorizer
    from sklearn.feature_extraction.text import TfidfTransformer
    from sklearn import svm
	
	text_clf = Pipeline([('vect', CountVectorizer()),
                     ('tfidf', TfidfTransformer()),
                     ('clf', svm.LinearSVC())
                    ])
    text_clf = text_clf.fit(X,y)
    y2 = text_clf.predict(X2)
    y2
```
### 模型优化（调参）

#### 网格搜索 [Grid Search](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html)
```python
	from sklearn.model_selection import GridSearchCV
	from sklearn.ensemble import RandomForestClassifier
	
	# 随机森林分类模型
	RFC = RandomForestClassifier()
	
	##设置备选属性用于grid search
	rf_param_grid = {"max_depth": [None],
				"max_features": [1, 3, 10],
				"min_samples_split": [2, 3, 10],
				"min_samples_leaf": [1, 3, 10],
				"bootstrap": [False],
				"n_estimators" :[100,300],
				"criterion": ["gini"]}
	#用于系统地遍历多种参数组合，通过交叉验证确定最佳效果参数
	gsRFC = GridSearchCV(RFC,param_grid = rf_param_grid, cv=kfold, scoring="accuracy", n_jobs= 4, verbose = 1)
	
	gsRFC.fit(X_train,Y_train)
	#得到最佳参数组合
	RFC_best = gsRFC.best_estimator_
	
	# Best score
	gsRFC.best_score_
```
#### 交叉验证 [Cross Validation](http://sklearn.apachecn.org/cn/0.19.0/modules/cross_validation.html)
```python
	#按比例划分样本（随机抽样）
	from sklearn.cross_validation import train_test_split
	X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42)  //X为样本、y为类别、test_size为测试集比例
	
	#通过cross_validation，设置cv=5，进行5折交叉验证，最后得到一个scores的预测准确率数组，表示每次交叉验证得到的准确率。
	clf = svm.SVC(kernel='linear', C=1)
	scores = cross_validation.cross_val_score(clf, X, y, cv=5)
	>>>array([ 0.96...,  1.  ...,  0.96...,  0.96...,  1.        ])
	
	print("Accuracy: %0.2f (+/- %0.2f)" % (scores.mean(), scores.std() * 2))
	>>>Accuracy: 0.98 (+/- 0.03)
```
[k折](https://ljalphabeta.gitbooks.io/python-/content/kfold.html)：
使用k折交叉验证来寻找最优参数要比holdout方法更稳定。一旦我们找到最优参数，要使用这组参数在原始数据集上训练模型作为最终的模型。
```python
	from sklearn.cross_validation import cross_val_score # K折交叉验证模块
	import matplotlib.pyplot as plt #可视化模块
	from sklearn.neighbors import KNeighborsClassifier # K最近邻(kNN，k-NearestNeighbor)分类算法
	
	#建立测试参数集
	k_range = range(1, 31)
	k_scores = []
	
	#借由迭代的方式来计算不同参数对模型的影响，并返回交叉验证后的平均准确率
	for k in k_range:
		knn = KNeighborsClassifier(n_neighbors=k)
		scores = cross_val_score(knn, X, y, cv=10, scoring='accuracy')
		k_scores.append(scores.mean())
		print('Fold %s, Acc: %.3f' %(k+1,scores))

	#可视化数据
	plt.plot(k_range, k_scores)
	plt.xlabel('Value of K for KNN')
	plt.ylabel('Cross-Validated Accuracy')
	plt.show()
```
### 输出

#### 表格输出
```python
    submission = pd.DataFrame({
        "PassengerId": test_df["PassengerId"],
        "Survived": Y_pred
    })
    submission.to_csv('C:/Users/caijiayue/Desktop/titanic2.csv', index=False)
```
#### One-hot后表格输出
```python
    from sklearn import preprocessing
    encoder = preprocessing.LabelBinarizer()
    encoder.fit(list(set(y2)))
    one_hot_labels = encoder.transform(y2)                         
	prediction = pd.DataFrame(one_hot_labels, columns=['EAP','HPL','MWS']).to_csv('C:/Users/caijiayue/Desktop/author-pre.csv')
```
	