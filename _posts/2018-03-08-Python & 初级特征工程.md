---
layout:     post
title:      Python & 初级特征工程
subtitle:   For Data Mining
date:       2018-03-08
author:     Jiayue Cai
header-img: img/post-bg-debug.png
catalog: true
tags:
    - Python
    - Feature Engineering
---


>Last updated on 2018-8-31... 

### 常用头文件 

	import numpy as np               #数学
	import pandas as pd              #表格
	from pandas import Series,DataFrame
    import matplotlib.pyplot as plt  #图形
    %matplotlib inline

	
### 特征工程（简单）

	df = pd.read_csv("data/train.csv")
	df.describe()                    #各项统计
	df.info()                        #看总数及数值类型 

#### 缺失值处理

对于竞赛而言最好不要直接删除，最好另作特殊编码

	df = df.drop(['PassengerId','Name','Ticket','Cabin'], axis=1)  #对于大量缺失数据的列可直接删除
	df = df.dropna()                                               #删除含有NaN数据的行
	df['Embarked'] = df['Embarked'].fillna('S')                    #离散值填充众数  
	median_age = train['Age'].median()                             #连续值填充中位数（或者平均值）
	df['Age'] = df['Age'].fillna(median_age)
	
#### 数据格式转换

其实就相当于简易的编码，需根据分类器的特性来，比如说树模型在求解分裂点的时候只考虑排序分位点。
GBDT更适合连续特征：在树分裂的时候，选择连续特征等于选择了一个特征；而选择离散特征时却等于选择了一个特征的某一维。
对于离散特征，要特征统计后分桶，将特征维度压缩到合理范围。（其实这出于效率，会降低精度，但一定程度上抑制了过拟合）

	df.loc[ (df.Sex == 'male'), 'Sex' ] = 0    #令男为0
	df.loc[ (df.Sex == 'female'), 'Sex' ] = 1  #令女为1
	df['Sex'] = df['Sex'].map( {'male': 0, 'female': 1} ).astype(int) #这种写法更好
	
#### 合并相似特征

	#1、称谓
	df['Title'] = df['Name'].str.extract('([A-Za-z]+)\.', expand=False)
    df['Title'] = df['Title'].fillna('NoTitle')
    df['Title'] = df['Title'].replace(['Lady', 'Countess','Capt', 'Col','Don', 'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona'], 'Rare')
    df['Title'] = df['Title'].replace(['Mlle','Ms'], 'Miss')
	df['Title'] = df['Title'].replace('Mme', 'Mrs')
	
	#2、亲戚数量与子女数量
    df['Companions'] = df['Parch'] + df['SibSp']
    to_be_dropped.extend(['Parch', 'SibSp'])

### 特征工程（进阶）

#### 单值连续特征(连续编码)

	from sklearn.preprocessing import OneHotEncoder,LabelEncoder
	one_hot_feature=['LBS','age','carrier','consumptionAbility','education','gender','house','os','ct','marriageStatus']
	for feature in one_hot_feature:   	          #LabelEncoder将各种标签分配一个可数的连续编号
		try:
			data[feature] = LabelEncoder().fit_transform(data[feature].apply(int))
		except:
			data[feature] = LabelEncoder().fit_transform(data[feature])
			
	enc = OneHotEncoder()                         #one-hot表示（这一步须结合分类器特性）
	for feature in one_hot_feature:
		enc.fit(data[feature].values.reshape(-1, 1))
		train_a=enc.transform(train[feature].values.reshape(-1, 1))
		test_a = enc.transform(test[feature].values.reshape(-1, 1))
		train_x= sparse.hstack((train_x, train_a))
		test_x = sparse.hstack((test_x, test_a))
	
	
#### 多值特征（top编码）

	from sklearn.preprocessing import LabelEncoder
	vecc = vec['interest1'].value_counts()[:2000]  #代表出现数量前2000名的兴趣子片段，调大可提高精度
	vecc_Fragment = vecc.index.tolist()
	vecc_laber_encoder = LabelEncoder().fit_transform(vecc_Fragment)
	vecci = dict(zip(vecc_Fragment,vecc_laber_encoder))
	
#### 统计特征

	#1、长度
	vector_feature=['interest1','interest2','interest5','kw1','kw2','topic1','topic2']
	for feat in vector_feature:                    
		data['len_'+feat] = data[feat].apply(lambda x:0 if ((not x.strip()) or str(x)=='-1') else len(x.split(' ')))
		
	#2、平均值
	data['meanlen_interest'] = (data['len_interest1']+data['len_interest2']+data['len_interest5'])/3.0
	
    #3、最大值
	data['max_ct'] = data['ct'].apply(lambda x: 0 if ((not str(x).strip()) or str(x)=='-1') else max(int(i) for i in str(x).split(' ')))  
	
	#4、分层
	df.loc[ df['Age'] <= 16, 'Age'] = 0
    df.loc[ (df['Age'] > 16) & (df['Age'] <= 32), 'Age'] = 1
    df.loc[ (df['Age'] > 32) & (df['Age'] <= 48), 'Age'] = 2
    df.loc[ (df['Age'] > 48) & (df['Age'] <= 64), 'Age'] = 3
    df.loc[ df['Age'] > 64, 'Age'] = 4
    df['Age'] = df['Age'].astype(int)
	
#### 转化率特征（命中率）

	num_ad = train['aid'].value_counts().sort_index()
	num_ad_clicked = data_clicked['aid'].value_counts().sort_index()
	ratio_ad_clicked = num_ad_clicked / num_ad
	ratio_ad_clicked = pd.DataFrame({
		'aid': ratio_ad_clicked.index,
		'ratio_ad_clicked' : ratio_ad_clicked.values
	})
	data = pd.merge(data, ratio_ad_clicked, on=['aid'], how='left')
	
#### 归一化

	#将新加入的特征归一化
	from sklearn.preprocessing import StandardScaler
	scaler = StandardScaler()
	scaler.fit(train[['ratio_ad_clicked', 'num_ad_push2user']].values)
	train_x = scaler.transform(train[['ratio_ad_clicked', 'num_ad_push2user']].values)
	
	
### 模型调用（简单）
	
	X_test = pd.read_csv("data/test.csv")
	from sklearn.svm import SVC, LinearSVC
	svc = SVC()
	svc.fit(df, dfs)
	Y_pred = svc.predict(X_test)
	svc.score(df, df.Survived)

### 模型调用（进阶）

	def LGB_predict(train_x,train_y,test_x,res):
		print("LGB test")
		clf = lgb.LGBMClassifier(
			boosting_type='gbdt', num_leaves=31, reg_alpha=0.0, reg_lambda=1,zero_as_missing=True,
			max_depth=-1, n_estimators=500, objective='binary',
			subsample=0.9, colsample_bytree=0.8, subsample_freq=1,
			learning_rate=0.1, min_child_weight=50, random_state=2018, n_jobs=100
		)
		clf.fit(train_x, train_y, eval_set=[(train_x, train_y)], eval_metric='auc',early_stopping_rounds=100)
		res['score'] = clf.predict_proba(test_x)[:,1]
		res['score'] = res['score'].apply(lambda x: float('%.6f' % x))
		res.to_csv('../data/submission.csv', index=False)
		os.system('zip baseline.zip ../data/submission.csv')
		return clf

	model=LGB_predict(train_x,train_y,test_x,res)
	
### 使用管道

    df = pd.read_csv('data/train.csv')
    y = df.author.values
    X = df.text.values

    df2 = pd.read_csv('data/test.csv')
    X2 = df2.text.values
	
	from sklearn.pipeline import Pipeline
    from sklearn.feature_extraction.text import CountVectorizer
    from sklearn.feature_extraction.text import TfidfTransformer
    from sklearn import svm
	
	text_clf = Pipeline([('vect', CountVectorizer()),
                     ('tfidf', TfidfTransformer()),
                     ('clf', svm.LinearSVC())
                    ])
    text_clf = text_clf.fit(X,y)
    y2 = text_clf.predict(X2)
    y2
	
### 模型优化（调参）

#### 网格搜索 [Grid Search](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html)

	from sklearn.model_selection import GridSearchCV
	from sklearn.ensemble import RandomForestClassifier
	
	# 随机森林分类模型
	RFC = RandomForestClassifier()
	
	##设置备选属性用于grid search
	rf_param_grid = {"max_depth": [None],
				"max_features": [1, 3, 10],
				"min_samples_split": [2, 3, 10],
				"min_samples_leaf": [1, 3, 10],
				"bootstrap": [False],
				"n_estimators" :[100,300],
				"criterion": ["gini"]}
	#用于系统地遍历多种参数组合，通过交叉验证确定最佳效果参数
	gsRFC = GridSearchCV(RFC,param_grid = rf_param_grid, cv=kfold, scoring="accuracy", n_jobs= 4, verbose = 1)
	
	gsRFC.fit(X_train,Y_train)
	#得到最佳参数组合
	RFC_best = gsRFC.best_estimator_
	
	# Best score
	gsRFC.best_score_

#### 交叉验证	
	
	#按比例划分样本（随机抽样）
	from sklearn.cross_validation import train_test_split
	X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42)  //X为样本、y为类别、test_size为测试集比例
	
	#通过cross_validation，设置cv=5，进行5倍交叉验证，最后得到一个scores的预测准确率数组，表示每次交叉验证得到的准确率。
	clf = svm.SVC(kernel='linear', C=1)
	scores = cross_validation.cross_val_score(clf, X, y, cv=5)
	>>>array([ 0.96...,  1.  ...,  0.96...,  0.96...,  1.        ])
	
	print("Accuracy: %0.2f (+/- %0.2f)" % (scores.mean(), scores.std() * 2))
	>>>Accuracy: 0.98 (+/- 0.03)
	
### 输出

#### 表格输出

    submission = pd.DataFrame({
        "PassengerId": test_df["PassengerId"],
        "Survived": Y_pred
    })
    submission.to_csv('C:/Users/caijiayue/Desktop/titanic2.csv', index=False)
	
#### One-hot后表格输出

    from sklearn import preprocessing
    encoder = preprocessing.LabelBinarizer()
    encoder.fit(list(set(y2)))
    one_hot_labels = encoder.transform(y2)                         
	prediction = pd.DataFrame(one_hot_labels, columns=['EAP','HPL','MWS']).to_csv('C:/Users/caijiayue/Desktop/author-pre.csv')