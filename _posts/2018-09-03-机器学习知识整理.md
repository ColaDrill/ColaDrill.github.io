---
layout:     post
title:      机器学习知识整理
subtitle:   For 面试
date:       2018-09-03
author:     Jiayue Cai
header-img: img/post-bg-debug.png
catalog: true
tags:
    - Machine Learning
---


>Last updated on 2018-10-09... 

整理自[>>原文链接](https://zhuanlan.zhihu.com/p/37762132?utm_source=qq&utm_medium=social&utm_oi=566394839504048128)，未完待续。

### 相关相似性度量

**欧式距离的平方**

我们一般用欧式距离来衡量向量的相似度，但是欧式距离无法考虑不同变量间取值的差异

e.g.变量a取值范围是0-1，b为0-10000，则b就相当于直接决定运算结果

**余弦相似度**

略

**皮尔森相关系数**

等价于z-score标准化后再欧式距离的平方

当样本数很少或者特征的取值范围更广时，更容易得出绝对值更大的皮尔森系数，所以样本量`不同或者取值范围不同的特征的之间`相关系数不一定可以做比较。

另外皮尔森相关系数`只能衡量线性相关性`，随机变量X和Y不相关并不意味二者独立。

当相关系数为0时我们知道的是线性分类器不能单利用这个特征的目前的形态做到将不同的类分开，但通过特征本身的变换、和其它特征组合使用或者与其它特征结合出新的特征却可能让它焕发出生机发挥出价值。

**三者关系**

欧式距离的平方 = 2 x 向量长度 x (1-pearson相关性系数)

在数据标准化（均值0，方差1）后，pearson相关性系数、Cosine相似度、欧式距离的平方可以认为是等价的。

```python
import numpy as np
from sklearning.preprocessing import StandardScaler
from scipy.spatial.distance import euclidean
from scipy.spatial.distance import cosine
from scipy.stats import pearsonr

n = 100 #向量长度
x1 = np.random.random_integers(0,10,(n,1))
x2 = np.random.random_integers(0,10,(n,1))

#标准化
x1_z = StandardScaler().fit_transform(x1)
x2_z = StandardScaler().fit_transform(x2)

e = (euclidean(x1_z,x2_z)**2) / (2*n)
c = cosine(x1_z,x2_z)
p = pearsonr(x1_z,x2_z)[0][0]
```

### 正负标签不平衡

处理的经验：一般解决方法是过采样，但是一般的过采样具有盲目性
- 基于邻域粗糙集的采样方法
- 半监督结合过采样

### 特征共线性

原特征的变换可能会存在共线性，处理的经验：
- 模型训练前：利用模型对特征排序，做topK筛选
- 模型训练时：随机选择特征和参数，来减弱特征共线性带来的影响。

### 评价指标

很多机器学习的模型对分类问题的预测结果都是概率，如果要计算`accuracy`，需要先把概率转化成类别，这就需要手动设置一个阈值，如果对一个样本的预测概率高于这个预测，就把这个样本放进一个类别里面，低于这个阈值，放进另一个类别里面。所以这个阈值很大程度上影响了accuracy的计算。
- 使用`AUC`或者`logloss`可以避免把预测概率转换成类别。
- AUC对样本类别是否均衡并不敏感，这也是`不均衡样本`通常用AUC评价分类器性能的一个原因。
- 可以直接优化AUC来训练分类器（[rankboost算法](http://papers.nips.cc/paper/2518-auc-optimization-vs-error-rate-minimization.pdf)）

AUC：Area under curve,曲线下面区域的面积。这个曲线叫`ROC曲线`。
从所有1样本中随机选取一个样本，从所有0样本中随机选取一个样本，然后根据你的分类器对两个随机样本进行预测，把1样本预测为1的概率为p1，把0样本预测为1的概率为p0，p1>p0的概率就等于AUC。

```python
from sklearn import metrics
def aucfun(act,pred):
  fpr, tpr, thresholds = metrics.roc_curve(act, pred, pos_label=1)
  return metrics.auc(fpr, tpr)
```

### k-d树

[>>原文](https://www.cnblogs.com/eyeszjwang/articles/2429382.html)

k-dimensional树的简称，是一种分割k维数据空间的数据结构。主要应用于多维空间关键数据的搜索（如：范围搜索和最近邻搜索）。步骤大致如下:
- 统计每个维上的数据方差，选取方差最大作为分割方向
- 按这个方向取中值，去该点并垂直于分割方向的平面作为分割超平面
- 超平面两侧便作为左右子空间，然后对左子空间和右子空间内的数据 重复根节点的过程 就可以得到下一级子节点

应用：SIFT算法中做`特征点匹配`的时候就会利用到k-d树。而特征点匹配实际上就是一个通过距离函数在高维矢量之间进行相似性检索的问题。针对如何快速而准确地找到查询点的近邻，现在提出了很多高维空间索引结构和近似查询的算法，k-d树就是其中一种。

当位数直接利用k-d树快速检索（维数不超过20）的性能急剧下降。假设数据集的维数为D，一般来说要求数据的规模N满足N»2D，才能达到高效的搜索。所以这就引出了一系列对k-d树算法的改进。有待进一步研究学习。


### 朴素贝叶斯 

[>>原文](https://zhuanlan.zhihu.com/p/26262151)）

前提：假设特征独立

**Sklearn**：
- 高斯模型：适用于多个类型变量，假设特征符合高斯分布。
- 多项式模型：用于离散计数。如一个句子中某个词语重复出现，我们视它们每个都是独立的，所以统计多次，概率指数上出现了次方。
- 伯努利模型：如果特征向量是二进制（即0和1），那这个模型是非常有用的。不同于多项式，伯努利把出现多次的词语视为只出现一次，更加简单方便。

**提升方法**：
- 如果连续特征不是正态分布的，我们应该使用各种不同的方法将其转换正态分布。
- 如果测试数据集具有“零频率”的问题（如果分类变量的类别（测试数据集）没有在训练数据集总被观察到，那这个模型会分配一个0概率给它，同时也会无法进行预测），应用[平滑技术“拉普拉斯估计”](https://zhuanlan.zhihu.com/p/26329951)修正数据集。
- 删除重复出现的高度相关的特征，可能会丢失频率信息，影响效果。
- 朴素贝叶斯分类在参数调整上选择有限。我建议把重点放在数据的预处理和特征选择。
- 大家可能想应用一些分类组合技术 如ensembling、bagging和boosting，但这些方法都于事无补。因为它们的目的是为了减少差异，朴素贝叶斯没有需要最小化的差异。
 
**应用**：

实时预测、多类预测、文本分类、垃圾邮件过滤、情感分析、推荐系统

注：Laplace校准（拉普拉斯平滑）：它的思想非常简单，就是对每个类别下所有划分的计数加1，这样如果训练样本集数量充分大时，并不会对结果产生影响，并且解决了上述频率为0的尴尬局面。
- 假设在文本分类中，有3个类，C1、C2、C3，在指定的训练样本中，某个词语K1，在各个类中观测计数分别为0，990，10，K1的概率为0，0.99，0.01，
- 对这三个量使用拉普拉斯平滑的计算方法如下：1/1003 = 0.001，991/1003=0.988，11/1003=0.011
- 在实际的使用中也经常使用加 lambda（1≥lambda≥0）来代替简单加1。如果对N个计数都加上lambda，这时分母也要记得加上N*lambda。

### SVM

[>>详细见原文](https://blog.csdn.net/szlcw1/article/details/52259668)

SVM是一种二类分类模型。它的基本模型是在特征空间中寻找间隔最大化的分离超平面的线性分类器。（间隔最大是它有别于感知机）
- 当训练样本线性可分时，通过硬间隔最大化，学习一个线性分类器，即线性可分支持向量机。
- 当训练数据近似线性可分时，引入松弛变量，通过软间隔最大化，学习一个线性分类器，即线性支持向量机。
- 当训练数据线性不可分时，通过使用核技巧及软间隔最大化，学习非线性支持向量机。

以上各SVM的数学推导应该熟悉：硬间隔最大化（几何间隔）---学习的对偶问题---软间隔最大化（引入松弛变量）---非线性支持向量机（核技巧）

为什么要将求解SVM的原始问题转换为其`对偶问题`？
- 1.对偶问题往往更易求解（当我们寻找约束存在时的最优点的时候，约束的存在虽然减小了需要搜寻的范围，但是却使问题变得更加复杂。为了使问题变得易于处理，我们的方法是把目标函数和约束全部融入一个新的函数，即拉格朗日函数，再通过这个函数来寻找最优点。）
- 2.自然引入核函数，进而推广到非线性分类问题。

为什么SVM要引入`核函数`？
- 当样本在原始空间线性不可分时，可将样本从原始空间映射到一个更高维的特征空间，使得样本在这个特征空间内线性可分。

为什么SVM对`缺失数据敏感`？
- 这里说的缺失数据是指缺失某些特征数据，向量数据不完整。SVM没有处理缺失值的策略（决策树有）。
- 而SVM希望样本在特征空间中线性可分，所以特征空间的好坏对SVM的性能很重要。缺失特征数据将影响训练结果的好坏。


### 随机森林

鉴于决策树容易过拟合的缺点，随机森林采用`多个决策树的投票机制`来改善决策树:

我们假设随机森林使用了m棵决策树，那么就需要产生m个一定数量的样本集来训练每一棵树，如果用全样本去训练m棵决策树显然是不可取的，全样本训练忽视了局部样本的规律，对于模型的泛化能力是有害的

产生n个样本的方法采用Bootstrap取样法，这是一种有放回的抽样方法，产生n个样本

而最终结果采用`Bagging的策略`来获得，即多数投票机制

**随机森林的生成方法**:

1.从样本集中通过重采样的方式产生n个样本

2.假设样本特征数目为a，对n个样本选择a中的k个特征，用建立决策树的方式获得最佳分割点

3.重复m次，产生m棵决策树

4.多数投票机制来进行预测
- 需要注意的一点是，这里m是指循环的次数，n是指样本的数目，n个样本构成训练的样本集，而m次循环中又会产生m个这样的样本集

**模型总结**:

随机森林是一个比较优秀的模型，它对于多维特征的数据集分类有很高的效率，还可以做特征重要性的选择。

运行效率和准确率较高，实现起来也比较简单。但是在数据噪音比较大的情况下会过拟合，过拟合的缺点对于随机森林来说还是较为致命的。

### 决策树的Boosting集成

#### 决策树与Boosting

**分类与回归树（CART）**:二叉树形式

- 分类时：根据Gini指数选择划分特征
- 回归时：以Los（交叉熵）为平方损失函数

最小化均方误差选择划分特征，切分点（值）将数据切分成两部分，用平方误差最小的准则求解每个单元上的最优输出值（每个叶子节点上的预测值为所有样本的平均值）。

**Boosting**:

> **让错分的样本权重越来越大，使它们被后面的树更加重视**

`Boosting` 每一步的残差计算其实变相地增大了分错instance的权重，而已经分对的instance则都趋向于0。这样后面的树就能越来越专注那些前面被分错的instance。

`Adaboost` 按分类对错，分配不同的weight，计算cost function时使用这些weight，从而让“错分的样本权重越来越大，使它们更被重视”。

#### GBDT、XGBoost、lightGBM

**GBDT（Gradient Boosting Decison Tree）**:

> **每一棵树学的是之前所有树结论和的残差，这个残差就是一个加预测值后能得真实值的累加量**

Gradient Boost与传统的Boost的区别是，每一次的计算是为了减少上一次的残差(residual)，而为了消除残差，我们可以在残差减少的梯度(Gradient)方向上建立一个新的模型。所以说，在Gradient Boost中，每个新的模型的建立是为了使得之前模型的残差往梯度方向减少。

Shrinkage（缩减）的思想认为，每次走一小步逐渐逼近结果的效果，要比每次迈一大步很快逼近结果的方式更容易避免过拟合。
即它不完全信任每一个棵残差树，它认为每棵树只学到了真理的一小部分，累加的时候只累加一小部分，通过多学几棵树弥补不足。
本质上，Shrinkage为每棵树设置了一个weight，累加时要乘以这个weight，但和Gradient并没有关系。

- 优点：它的非线性变换比较多，表达能力强，而且不需要做复杂的特征工程和特征变换。
- 缺点: Boost是一个串行过程，不好并行化，而且计算复杂度高，同时不太适合高维稀疏特征。

**XGBoost**:

> **能自动利用cpu的多线程，而且适当改进了gradient boosting，加了剪枝，控制了模型的复杂程度**

传统GBDT以CART作为基分类器，xgboost还支持线性分类器，这个时候xgboost相当于带L1和L2正则化项的逻辑斯蒂回归（分类问题）或者线性回归（回归问题）。

传统GBDT在优化时只用到一阶导数信息，xgboost则对代价函数进行了二阶泰勒展开，同时用到了一阶和二阶导数。（xgboost工具支持自定义代价函数，只要函数可一阶和二阶求导。）

它的并行是在特征粒度上的：我们知道，决策树的学习最耗时的一个步骤就是对特征的值进行排序（因为要确定最佳分割点），xgboost在训练之前，预先对数据进行了排序，然后保存为block结构，后面的迭代中重复地使用这个结构，大大减小计算量。
这个block结构也使得并行成为了可能，在进行节点的分裂时，需要计算每个特征的增益，最终选增益最大的那个特征去做分裂，那么各个特征的增益计算就可以开多线程进行。

可并行的近似直方图算法：树节点在进行分裂时，我们需要计算每个特征的每个分割点对应的增益，即用贪心法枚举所有可能的分割点。
当数据无法一次载入内存或者在分布式情况下，贪心算法效率就会变得很低，所以xgboost还提出了一种可并行的近似直方图算法，用于高效地生成候选的分割点。

**lightGBM**：

> **基于决策树算法的分布式梯度提升框架**

关于数据分割点：

XGBoost使用的是pre-sorted算法（对所有特征都按照特征的数值进行预排序，在遍历分割点的时候用O(data)的代价找到一个特征上的最好分割点），能够更精确的找到数据分隔点

LightGBM使用的是histogram算法，占用的内存更低，数据分隔的复杂度更低

关于决策树的生长策略：

XGBoost采用的是level-wise生长策略，能够同时分裂同一层的叶子，从而进行多线程优化，不容易过拟合；但不加区分的对待同一层的叶子，带来了很多没必要的开销（因为实际上很多叶子的分裂增益较低，没必要进行搜索和分裂）

LightGBM采用leaf-wise生长策略，每次从当前所有叶子中找到分裂增益最大（一般也是数据量最大）的一个叶子，然后分裂，如此循环；但会生长出比较深的决策树，产生过拟合（因此LightGBM 在leaf-wise之上增加了一个最大深度的限制，在保证高效率的同时防止过拟合）。

另一个比较巧妙的优化是 histogram 做差加速。一个容易观察到的现象：一个叶子的直方图可以由它的父亲节点的直方图与它兄弟的直方图做差得到。








