---
layout:     post
title:      机器学习知识整理
subtitle:   For 面试
date:       2018-09-03
author:     Jiayue Cai
header-img: img/post-bg-debug.png
catalog: true
tags:
    - Machine Learning
---


>Last updated on 2018-10-09... 

整理自[>>原文链接](https://zhuanlan.zhihu.com/p/37762132?utm_source=qq&utm_medium=social&utm_oi=566394839504048128)，未完待续。

### 相关相似性度量

**欧式距离的平方**

我们一般用欧式距离来衡量向量的相似度，但是欧式距离无法考虑不同变量间取值的差异

e.g.变量a取值范围是0-1，b为0-10000，则b就相当于直接决定运算结果

**余弦相似度**

略

**皮尔森相关系数**

等价于z-score标准化后再欧式距离的平方

当样本数很少或者特征的取值范围更广时，更容易得出绝对值更大的皮尔森系数，所以样本量`不同或者取值范围不同的特征的之间`相关系数不一定可以做比较。

另外皮尔森相关系数`只能衡量线性相关性`，随机变量X和Y不相关并不意味二者独立。

当相关系数为0时我们知道的是线性分类器不能单利用这个特征的目前的形态做到将不同的类分开，但通过特征本身的变换、和其它特征组合使用或者与其它特征结合出新的特征却可能让它焕发出生机发挥出价值。

**三者关系**

欧式距离的平方 = 2 x 向量长度 x (1-pearson相关性系数)

在数据标准化（均值0，方差1）后，pearson相关性系数、Cosine相似度、欧式距离的平方可以认为是等价的。

```python
import numpy as np
from sklearning.preprocessing import StandardScaler
from scipy.spatial.distance import euclidean
from scipy.spatial.distance import cosine
from scipy.stats import pearsonr

n = 100 #向量长度
x1 = np.random.random_integers(0,10,(n,1))
x2 = np.random.random_integers(0,10,(n,1))

#标准化
x1_z = StandardScaler().fit_transform(x1)
x2_z = StandardScaler().fit_transform(x2)

e = (euclidean(x1_z,x2_z)**2) / (2*n)
c = cosine(x1_z,x2_z)
p = pearsonr(x1_z,x2_z)[0][0]
```

### 正负标签不平衡

处理的经验：一般解决方法是过采样，但是一般的过采样具有盲目性
- 基于邻域粗糙集的采样方法
- 半监督结合过采样

### 特征共线性

原特征的变换可能会存在共线性，处理的经验：
- 模型训练前：利用模型对特征排序，做topK筛选
- 模型训练时：随机选择特征和参数，来减弱特征共线性带来的影响。

### 评价指标

很多机器学习的模型对分类问题的预测结果都是概率，如果要计算`accuracy`，需要先把概率转化成类别，这就需要手动设置一个阈值，如果对一个样本的预测概率高于这个预测，就把这个样本放进一个类别里面，低于这个阈值，放进另一个类别里面。所以这个阈值很大程度上影响了accuracy的计算。
- 使用`AUC`或者`logloss`可以避免把预测概率转换成类别。
- AUC对样本类别是否均衡并不敏感，这也是`不均衡样本`通常用AUC评价分类器性能的一个原因。
- 可以直接优化AUC来训练分类器（[rankboost算法](http://papers.nips.cc/paper/2518-auc-optimization-vs-error-rate-minimization.pdf)）

AUC：Area under curve,曲线下面区域的面积。这个曲线叫`ROC曲线`。
从所有1样本中随机选取一个样本，从所有0样本中随机选取一个样本，然后根据你的分类器对两个随机样本进行预测，把1样本预测为1的概率为p1，把0样本预测为1的概率为p0，p1>p0的概率就等于AUC。

```python
from sklearn import metrics
def aucfun(act,pred):
  fpr, tpr, thresholds = metrics.roc_curve(act, pred, pos_label=1)
  return metrics.auc(fpr, tpr)
```

### k-d树

[>>原文](https://www.cnblogs.com/eyeszjwang/articles/2429382.html)

k-dimensional树的简称，是一种分割k维数据空间的数据结构。主要应用于多维空间关键数据的搜索（如：范围搜索和最近邻搜索）。步骤大致如下:
- 统计每个维上的数据方差，选取方差最大作为分割方向
- 按这个方向取中值，去该点并垂直于分割方向的平面作为分割超平面
- 超平面两侧便作为左右子空间，然后对左子空间和右子空间内的数据 重复根节点的过程 就可以得到下一级子节点

应用：SIFT算法中做`特征点匹配`的时候就会利用到k-d树。而特征点匹配实际上就是一个通过距离函数在高维矢量之间进行相似性检索的问题。针对如何快速而准确地找到查询点的近邻，现在提出了很多高维空间索引结构和近似查询的算法，k-d树就是其中一种。

当位数直接利用k-d树快速检索（维数不超过20）的性能急剧下降。假设数据集的维数为D，一般来说要求数据的规模N满足N»2D，才能达到高效的搜索。所以这就引出了一系列对k-d树算法的改进。有待进一步研究学习。


### 朴素贝叶斯 

[>>原文](https://zhuanlan.zhihu.com/p/26262151)）

前提：假设特征独立

**Sklearn**：
- 高斯模型：适用于多个类型变量，假设特征符合高斯分布。
- 多项式模型：用于离散计数。如一个句子中某个词语重复出现，我们视它们每个都是独立的，所以统计多次，概率指数上出现了次方。
- 伯努利模型：如果特征向量是二进制（即0和1），那这个模型是非常有用的。不同于多项式，伯努利把出现多次的词语视为只出现一次，更加简单方便。

**提升方法**：
- 如果连续特征不是正态分布的，我们应该使用各种不同的方法将其转换正态分布。
- 如果测试数据集具有“零频率”的问题（如果分类变量的类别（测试数据集）没有在训练数据集总被观察到，那这个模型会分配一个0概率给它，同时也会无法进行预测），应用[平滑技术“拉普拉斯估计”](https://zhuanlan.zhihu.com/p/26329951)修正数据集。
- 删除重复出现的高度相关的特征，可能会丢失频率信息，影响效果。
- 朴素贝叶斯分类在参数调整上选择有限。我建议把重点放在数据的预处理和特征选择。
- 大家可能想应用一些分类组合技术 如ensembling、bagging和boosting，但这些方法都于事无补。因为它们的目的是为了减少差异，朴素贝叶斯没有需要最小化的差异。
 
**应用**：

实时预测、多类预测、文本分类、垃圾邮件过滤、情感分析、推荐系统

注：Laplace校准（拉普拉斯平滑）：它的思想非常简单，就是对每个类别下所有划分的计数加1，这样如果训练样本集数量充分大时，并不会对结果产生影响，并且解决了上述频率为0的尴尬局面。
- 假设在文本分类中，有3个类，C1、C2、C3，在指定的训练样本中，某个词语K1，在各个类中观测计数分别为0，990，10，K1的概率为0，0.99，0.01，
- 对这三个量使用拉普拉斯平滑的计算方法如下：1/1003 = 0.001，991/1003=0.988，11/1003=0.011
- 在实际的使用中也经常使用加 lambda（1≥lambda≥0）来代替简单加1。如果对N个计数都加上lambda，这时分母也要记得加上N*lambda。

### SVM

[>>详细见原文](https://blog.csdn.net/szlcw1/article/details/52259668)

SVM是一种二类分类模型。它的基本模型是在特征空间中寻找间隔最大化的分离超平面的线性分类器。（间隔最大是它有别于感知机）
- 当训练样本线性可分时，通过硬间隔最大化，学习一个线性分类器，即线性可分支持向量机。
- 当训练数据近似线性可分时，引入松弛变量，通过软间隔最大化，学习一个线性分类器，即线性支持向量机。
- 当训练数据线性不可分时，通过使用核技巧及软间隔最大化，学习非线性支持向量机。

以上各SVM的数学推导应该熟悉：硬间隔最大化（几何间隔）---学习的对偶问题---软间隔最大化（引入松弛变量）---非线性支持向量机（核技巧）

为什么要将求解SVM的原始问题转换为其`对偶问题`？
- 1.对偶问题往往更易求解（当我们寻找约束存在时的最优点的时候，约束的存在虽然减小了需要搜寻的范围，但是却使问题变得更加复杂。为了使问题变得易于处理，我们的方法是把目标函数和约束全部融入一个新的函数，即拉格朗日函数，再通过这个函数来寻找最优点。）
- 2.自然引入核函数，进而推广到非线性分类问题。

为什么SVM要引入`核函数`？
- 当样本在原始空间线性不可分时，可将样本从原始空间映射到一个更高维的特征空间，使得样本在这个特征空间内线性可分。

为什么SVM对`缺失数据敏感`？
- 这里说的缺失数据是指缺失某些特征数据，向量数据不完整。SVM没有处理缺失值的策略（决策树有）。
- 而SVM希望样本在特征空间中线性可分，所以特征空间的好坏对SVM的性能很重要。缺失特征数据将影响训练结果的好坏。


### 随机森林

鉴于决策树容易过拟合的缺点，随机森林采用`多个决策树的投票机制`来改善决策树:

我们假设随机森林使用了m棵决策树，那么就需要产生m个一定数量的样本集来训练每一棵树，如果用全样本去训练m棵决策树显然是不可取的，全样本训练忽视了局部样本的规律，对于模型的泛化能力是有害的

产生n个样本的方法采用Bootstrap取样法，这是一种有放回的抽样方法，产生n个样本

而最终结果采用Bagging的策略来获得，即多数投票机制

**随机森林的生成方法**:

1.从样本集中通过重采样的方式产生n个样本

2.假设样本特征数目为a，对n个样本选择a中的k个特征，用建立决策树的方式获得最佳分割点

3.重复m次，产生m棵决策树

4.多数投票机制来进行预测
- 需要注意的一点是，这里m是指循环的次数，n是指样本的数目，n个样本构成训练的样本集，而m次循环中又会产生m个这样的样本集

**模型总结**:

随机森林是一个比较优秀的模型，它对于多维特征的数据集分类有很高的效率，还可以做特征重要性的选择。

运行效率和准确率较高，实现起来也比较简单。但是在数据噪音比较大的情况下会过拟合，过拟合的缺点对于随机森林来说还是较为致命的。

