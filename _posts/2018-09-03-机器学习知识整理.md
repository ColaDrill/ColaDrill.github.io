---
layout:     post
title:      机器学习知识整理
subtitle:   For 面试
date:       2018-09-03
author:     Jiayue Cai
header-img: img/post-bg-debug.png
catalog: true
tags:
    - Machine Learning
---


>Last updated on 2018-9-3... 

整理自[>>原文链接](https://zhuanlan.zhihu.com/p/37762132?utm_source=qq&utm_medium=social&utm_oi=566394839504048128)，未完待续。

### 朴素贝叶斯 

![bayes](https://pic4.zhimg.com/80/v2-15b16ce6d37b616a5443c0f7e42e03ec_hd.jpg)
上图可以把A理解为特征，B理解为类别。（[>>原文](https://zhuanlan.zhihu.com/p/26262151)）

前提：假设特征独立

Sklearn：
- 高斯模型：适用于多个类型变量，假设特征符合高斯分布。
- 多项式模型：用于离散计数。如一个句子中某个词语重复出现，我们视它们每个都是独立的，所以统计多次，概率指数上出现了次方。
- 伯努利模型：如果特征向量是二进制（即0和1），那这个模型是非常有用的。不同于多项式，伯努利把出现多次的词语视为只出现一次，更加简单方便。

提升方法：
- 如果连续特征不是正态分布的，我们应该使用各种不同的方法将其转换正态分布。
- 如果测试数据集具有“零频率”的问题（如果分类变量的类别（测试数据集）没有在训练数据集总被观察到，那这个模型会分配一个0概率给它，同时也会无法进行预测），应用[平滑技术“拉普拉斯估计”](https://zhuanlan.zhihu.com/p/26329951)修正数据集。
- 删除重复出现的高度相关的特征，可能会丢失频率信息，影响效果。
- 朴素贝叶斯分类在参数调整上选择有限。我建议把重点放在数据的预处理和特征选择。
- 大家可能想应用一些分类组合技术 如ensembling、bagging和boosting，但这些方法都于事无补。因为它们的目的是为了减少差异，朴素贝叶斯没有需要最小化的差异。

应用：实时预测、多类预测、文本分类、垃圾邮件过滤、情感分析、推荐系统

>注：Laplace校准（拉普拉斯平滑）：它的思想非常简单，就是对每个类别下所有划分的计数加1，这样如果训练样本集数量充分大时，并不会对结果产生影响，并且解决了上述频率为0的尴尬局面。
假设在文本分类中，有3个类，C1、C2、C3，在指定的训练样本中，某个词语K1，在各个类中观测计数分别为0，990，10，K1的概率为0，0.99，0.01，
对这三个量使用拉普拉斯平滑的计算方法如下：1/1003 = 0.001，991/1003=0.988，11/1003=0.011
在实际的使用中也经常使用加 lambda（1≥lambda≥0）来代替简单加1。如果对N个计数都加上lambda，这时分母也要记得加上N*lambda。

```python
	import numpy as np
	print("hello")
```