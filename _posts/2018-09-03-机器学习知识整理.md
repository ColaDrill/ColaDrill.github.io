---
layout:     post
title:      机器学习知识整理
subtitle:   For 面试
date:       2018-09-03
author:     Jiayue Cai
header-img: img/post-bg-debug.png
catalog: true
tags:
    - Machine Learning
---


>Last updated on 2018-9-23... 

整理自[>>原文链接](https://zhuanlan.zhihu.com/p/37762132?utm_source=qq&utm_medium=social&utm_oi=566394839504048128)，未完待续。

### 评价指标

很多机器学习的模型对分类问题的预测结果都是概率，如果要计算`accuracy`，需要先把概率转化成类别，这就需要手动设置一个阈值，如果对一个样本的预测概率高于这个预测，就把这个样本放进一个类别里面，低于这个阈值，放进另一个类别里面。所以这个阈值很大程度上影响了accuracy的计算。
- 使用`AUC`或者`logloss`可以避免把预测概率转换成类别。
- AUC对样本类别是否均衡并不敏感，这也是`不均衡样本`通常用AUC评价分类器性能的一个原因。
- 可以直接优化AUC来训练分类器（[rankboost算法](http://papers.nips.cc/paper/2518-auc-optimization-vs-error-rate-minimization.pdf)）

AUC：Area under curve,曲线下面区域的面积。这个曲线叫`ROC曲线`。
从所有1样本中随机选取一个样本，从所有0样本中随机选取一个样本，然后根据你的分类器对两个随机样本进行预测，把1样本预测为1的概率为p1，把0样本预测为1的概率为p0，p1>p0的概率就等于AUC。

```python
from sklearn import metrics
def aucfun(act,pred):
  fpr, tpr, thresholds = metrics.roc_curve(act, pred, pos_label=1)
  return metrics.auc(fpr, tpr)
```

### k-d树

[>>原文](https://www.cnblogs.com/eyeszjwang/articles/2429382.html)

k-dimensional树的简称，是一种分割k维数据空间的数据结构。主要应用于多维空间关键数据的搜索（如：范围搜索和最近邻搜索）。步骤大致如下:
- 统计每个维上的数据方差，选取方差最大作为分割方向
- 按这个方向取中值，去该点并垂直于分割方向的平面作为分割超平面
- 超平面两侧便作为左右子空间，然后对左子空间和右子空间内的数据 重复根节点的过程 就可以得到下一级子节点

应用：SIFT算法中做`特征点匹配`的时候就会利用到k-d树。而特征点匹配实际上就是一个通过距离函数在高维矢量之间进行相似性检索的问题。针对如何快速而准确地找到查询点的近邻，现在提出了很多高维空间索引结构和近似查询的算法，k-d树就是其中一种。

当位数直接利用k-d树快速检索（维数不超过20）的性能急剧下降。假设数据集的维数为D，一般来说要求数据的规模N满足N»2D，才能达到高效的搜索。所以这就引出了一系列对k-d树算法的改进。有待进一步研究学习。


### 朴素贝叶斯 

[>>原文](https://zhuanlan.zhihu.com/p/26262151)）

前提：假设特征独立

Sklearn：
- 高斯模型：适用于多个类型变量，假设特征符合高斯分布。
- 多项式模型：用于离散计数。如一个句子中某个词语重复出现，我们视它们每个都是独立的，所以统计多次，概率指数上出现了次方。
- 伯努利模型：如果特征向量是二进制（即0和1），那这个模型是非常有用的。不同于多项式，伯努利把出现多次的词语视为只出现一次，更加简单方便。

提升方法：
- 如果连续特征不是正态分布的，我们应该使用各种不同的方法将其转换正态分布。
- 如果测试数据集具有“零频率”的问题（如果分类变量的类别（测试数据集）没有在训练数据集总被观察到，那这个模型会分配一个0概率给它，同时也会无法进行预测），应用[平滑技术“拉普拉斯估计”](https://zhuanlan.zhihu.com/p/26329951)修正数据集。
- 删除重复出现的高度相关的特征，可能会丢失频率信息，影响效果。
- 朴素贝叶斯分类在参数调整上选择有限。我建议把重点放在数据的预处理和特征选择。
- 大家可能想应用一些分类组合技术 如ensembling、bagging和boosting，但这些方法都于事无补。因为它们的目的是为了减少差异，朴素贝叶斯没有需要最小化的差异。

应用：实时预测、多类预测、文本分类、垃圾邮件过滤、情感分析、推荐系统

注：Laplace校准（拉普拉斯平滑）：它的思想非常简单，就是对每个类别下所有划分的计数加1，这样如果训练样本集数量充分大时，并不会对结果产生影响，并且解决了上述频率为0的尴尬局面。
- 假设在文本分类中，有3个类，C1、C2、C3，在指定的训练样本中，某个词语K1，在各个类中观测计数分别为0，990，10，K1的概率为0，0.99，0.01，
- 对这三个量使用拉普拉斯平滑的计算方法如下：1/1003 = 0.001，991/1003=0.988，11/1003=0.011
- 在实际的使用中也经常使用加 lambda（1≥lambda≥0）来代替简单加1。如果对N个计数都加上lambda，这时分母也要记得加上N*lambda。
